{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_<21400217>.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksYz3LNGShqe"
      },
      "source": [
        "#import libraries\n",
        "\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torchvision            \n",
        "import torch.nn as nn\n",
        "\n"
      ],
      "execution_count": 986,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLDv4L6fSiuj",
        "outputId": "5edf66c6-92a6-484c-bb14-406cfb9dc926",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "# import file\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n"
      ],
      "execution_count": 987,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-367d49ed-c678-435b-bad1-a51792b9d76f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-367d49ed-c678-435b-bad1-a51792b9d76f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving student-mat.csv to student-mat (1).csv\n",
            "User uploaded file \"student-mat.csv\" with length 56993 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ioj5wTFaUCoR"
      },
      "source": [
        "# read file into dataframe\n",
        "math_data = pd.read_csv(\"student-mat.csv\", sep=';')"
      ],
      "execution_count": 2250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9aLP18j113"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s58WdnlYTCE"
      },
      "source": [
        "#one-hot encoding & convert to numpy array\n",
        "data_encode = pd.get_dummies(math_data)\n",
        "\n",
        "#drop G1, G2 comlumn\n",
        "data_encode = data_encode.drop(['G1', 'G2'], axis=1)\n",
        "\n",
        "feature_count = len(data_encode.columns.tolist()) - 1\n"
      ],
      "execution_count": 2251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQs15zRLVj_6",
        "outputId": "63ffb4cb-acd3-44c0-f48f-8792297a1097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# showing data with boxplot\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "df = data_encode\n",
        "df = df.reindex(columns=['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', \n",
        "                         'school_GP', 'school_MS', 'sex_F', 'sex_M', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', \n",
        "                         'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher',\n",
        "                         'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'schoolsup_no', 'schoolsup_yes',\n",
        "                         'famsup_no', 'famsup_yes', 'paid_no', 'paid_yes', 'activities_no', 'activities_yes', 'nursery_no', 'nursery_yes', 'higher_no', 'higher_yes', \n",
        "                         'internet_no', 'internet_yes', 'romantic_no', 'romantic_yes', 'G3'])\n",
        "\n",
        "df.boxplot(figsize=(20,12))\n",
        "df\n",
        "\n"
      ],
      "execution_count": 2252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>Medu</th>\n",
              "      <th>Fedu</th>\n",
              "      <th>traveltime</th>\n",
              "      <th>studytime</th>\n",
              "      <th>failures</th>\n",
              "      <th>famrel</th>\n",
              "      <th>freetime</th>\n",
              "      <th>goout</th>\n",
              "      <th>Dalc</th>\n",
              "      <th>Walc</th>\n",
              "      <th>health</th>\n",
              "      <th>absences</th>\n",
              "      <th>school_GP</th>\n",
              "      <th>school_MS</th>\n",
              "      <th>sex_F</th>\n",
              "      <th>sex_M</th>\n",
              "      <th>address_R</th>\n",
              "      <th>address_U</th>\n",
              "      <th>famsize_GT3</th>\n",
              "      <th>famsize_LE3</th>\n",
              "      <th>Pstatus_A</th>\n",
              "      <th>Pstatus_T</th>\n",
              "      <th>Mjob_at_home</th>\n",
              "      <th>Mjob_health</th>\n",
              "      <th>Mjob_other</th>\n",
              "      <th>Mjob_services</th>\n",
              "      <th>Mjob_teacher</th>\n",
              "      <th>Fjob_at_home</th>\n",
              "      <th>Fjob_health</th>\n",
              "      <th>Fjob_other</th>\n",
              "      <th>Fjob_services</th>\n",
              "      <th>Fjob_teacher</th>\n",
              "      <th>reason_course</th>\n",
              "      <th>reason_home</th>\n",
              "      <th>reason_other</th>\n",
              "      <th>reason_reputation</th>\n",
              "      <th>guardian_father</th>\n",
              "      <th>guardian_mother</th>\n",
              "      <th>guardian_other</th>\n",
              "      <th>schoolsup_no</th>\n",
              "      <th>schoolsup_yes</th>\n",
              "      <th>famsup_no</th>\n",
              "      <th>famsup_yes</th>\n",
              "      <th>paid_no</th>\n",
              "      <th>paid_yes</th>\n",
              "      <th>activities_no</th>\n",
              "      <th>activities_yes</th>\n",
              "      <th>nursery_no</th>\n",
              "      <th>nursery_yes</th>\n",
              "      <th>higher_no</th>\n",
              "      <th>higher_yes</th>\n",
              "      <th>internet_no</th>\n",
              "      <th>internet_yes</th>\n",
              "      <th>romantic_no</th>\n",
              "      <th>romantic_yes</th>\n",
              "      <th>G3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>395 rows × 57 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  Medu  Fedu  traveltime  ...  internet_yes  romantic_no  romantic_yes  G3\n",
              "0     18     4     4           2  ...             0            1             0   6\n",
              "1     17     1     1           1  ...             1            1             0   6\n",
              "2     15     1     1           1  ...             1            1             0  10\n",
              "3     15     4     2           1  ...             1            0             1  15\n",
              "4     16     3     3           1  ...             0            1             0  10\n",
              "..   ...   ...   ...         ...  ...           ...          ...           ...  ..\n",
              "390   20     2     2           1  ...             0            1             0   9\n",
              "391   17     3     1           2  ...             1            1             0  16\n",
              "392   21     1     1           1  ...             0            1             0   7\n",
              "393   18     3     2           3  ...             1            1             0  10\n",
              "394   19     1     1           1  ...             1            1             0   9\n",
              "\n",
              "[395 rows x 57 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2252
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtIyVay_9LSE"
      },
      "source": [
        "# Removing outliers iteratively\n",
        "\n",
        "col_list = df.columns.to_list()\n",
        "for col in col_list:\n",
        "    if col == 'G3':\n",
        "        continue\n",
        "    if col == 'absences':\n",
        "        Q1 = df[col].quantile(0.10)\n",
        "        Q3 = df[col].quantile(0.70)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        filter = (df[col] >= Q1 - 1 * IQR) & (df[col] <= Q3 + 1 *IQR)\n",
        "        df = df.loc[filter]\n",
        "        continue\n",
        "\n",
        "    Q1 = df[col].quantile(0.10)\n",
        "    Q3 = df[col].quantile(0.90)\n",
        "    IQR = Q3 - Q1    \n",
        "\n",
        "    filter = (df[col] >= Q1 - 1.5 * IQR) & (df[col] <= Q3 + 1.5 *IQR)\n",
        "    df = df.loc[filter] \n"
      ],
      "execution_count": 2253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt4jQBOlDVXJ",
        "outputId": "67fa9763-e936-4f3a-e90b-21201fe47221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        }
      },
      "source": [
        "#box plot after outliers removed\n",
        "\n",
        "zscore_threshold = 1.8 \n",
        "\t\t\t  \n",
        "df.boxplot(figsize=(20,12))"
      ],
      "execution_count": 2254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa930e59b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2254
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1KiMlzpf1F5",
        "outputId": "85308462-5fec-48cb-9606-624368e25e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#make missing values\n",
        "row_list = df.index.to_list()\n",
        "row_count = len(row_list)\n",
        "random_index = np.random.rand(5) * row_count\n",
        "\n",
        "# Handle missing Values\n",
        "for i in random_index:\n",
        "    df.loc[row_list[int(i)], 'traveltime'] = np.nan\n",
        "    print(df.loc[[row_list[int(i)]]])\n",
        "df = df.fillna(1)\n",
        "print(df['traveltime'])"
      ],
      "execution_count": 2255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     age  Medu  Fedu  traveltime  ...  internet_yes  romantic_no  romantic_yes  G3\n",
            "124   16     2     2         NaN  ...             1            0             1   8\n",
            "\n",
            "[1 rows x 57 columns]\n",
            "     age  Medu  Fedu  traveltime  ...  internet_yes  romantic_no  romantic_yes  G3\n",
            "193   16     3     3         NaN  ...             1            1             0  10\n",
            "\n",
            "[1 rows x 57 columns]\n",
            "     age  Medu  Fedu  traveltime  ...  internet_yes  romantic_no  romantic_yes  G3\n",
            "318   17     3     4         NaN  ...             1            1             0  10\n",
            "\n",
            "[1 rows x 57 columns]\n",
            "     age  Medu  Fedu  traveltime  ...  internet_yes  romantic_no  romantic_yes  G3\n",
            "258   18     2     1         NaN  ...             1            1             0  14\n",
            "\n",
            "[1 rows x 57 columns]\n",
            "    age  Medu  Fedu  traveltime  ...  internet_yes  romantic_no  romantic_yes  G3\n",
            "83   15     2     2         NaN  ...             1            1             0  15\n",
            "\n",
            "[1 rows x 57 columns]\n",
            "1      1.0\n",
            "4      1.0\n",
            "5      1.0\n",
            "6      1.0\n",
            "9      1.0\n",
            "      ... \n",
            "387    1.0\n",
            "388    1.0\n",
            "389    2.0\n",
            "391    2.0\n",
            "393    3.0\n",
            "Name: traveltime, Length: 183, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVYQj82uXakZ",
        "outputId": "6d7351dd-9b70-410a-8fc8-2345b589d48a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#convert to numpy\n",
        "data_num = df.to_numpy()\n",
        "\n",
        "print(data_num.shape)\n",
        "\n",
        "# split into X and y\n",
        "X = data_num[:, :feature_count]\n",
        "y = data_num[:,feature_count]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 2256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(183, 57)\n",
            "(183, 56)\n",
            "(183,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-9sJgzzVwxk"
      },
      "source": [
        "## check feature distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XezLYSmWVzLk",
        "outputId": "c54ff833-bebf-4bc4-f624-f4f4f7b714b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(0,56):\n",
        "    print(\"scatter of feature\", i)\n",
        "    #plt.scatter(X[:,i], y)\n",
        "    #plt.ylim(-1,21)\n",
        "    #plt.xlim(np.min(X[:,i]-1), np.max(X[:,i]+1))\n",
        "    #plt.show()\n",
        "    \n",
        "    #mglearn.discrete_scatter(X[:, i],y )"
      ],
      "execution_count": 2257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scatter of feature 0\n",
            "scatter of feature 1\n",
            "scatter of feature 2\n",
            "scatter of feature 3\n",
            "scatter of feature 4\n",
            "scatter of feature 5\n",
            "scatter of feature 6\n",
            "scatter of feature 7\n",
            "scatter of feature 8\n",
            "scatter of feature 9\n",
            "scatter of feature 10\n",
            "scatter of feature 11\n",
            "scatter of feature 12\n",
            "scatter of feature 13\n",
            "scatter of feature 14\n",
            "scatter of feature 15\n",
            "scatter of feature 16\n",
            "scatter of feature 17\n",
            "scatter of feature 18\n",
            "scatter of feature 19\n",
            "scatter of feature 20\n",
            "scatter of feature 21\n",
            "scatter of feature 22\n",
            "scatter of feature 23\n",
            "scatter of feature 24\n",
            "scatter of feature 25\n",
            "scatter of feature 26\n",
            "scatter of feature 27\n",
            "scatter of feature 28\n",
            "scatter of feature 29\n",
            "scatter of feature 30\n",
            "scatter of feature 31\n",
            "scatter of feature 32\n",
            "scatter of feature 33\n",
            "scatter of feature 34\n",
            "scatter of feature 35\n",
            "scatter of feature 36\n",
            "scatter of feature 37\n",
            "scatter of feature 38\n",
            "scatter of feature 39\n",
            "scatter of feature 40\n",
            "scatter of feature 41\n",
            "scatter of feature 42\n",
            "scatter of feature 43\n",
            "scatter of feature 44\n",
            "scatter of feature 45\n",
            "scatter of feature 46\n",
            "scatter of feature 47\n",
            "scatter of feature 48\n",
            "scatter of feature 49\n",
            "scatter of feature 50\n",
            "scatter of feature 51\n",
            "scatter of feature 52\n",
            "scatter of feature 53\n",
            "scatter of feature 54\n",
            "scatter of feature 55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cCE7dCQddpx",
        "outputId": "be993038-b069-4fbd-87d5-0a633c2cd11d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#drop columns from X by making new dataset\n",
        "\n",
        "X_new = X[:,1]\n",
        "X_new = np.expand_dims(X_new,1)\n",
        "\n",
        "X_new = np.append(X_new, X[:, 3:4], axis=1)\n",
        "X_new = np.append(X_new, X[:, 5:6], axis=1)\n",
        "X_new = np.append(X_new, X[:, 8:9], axis=1)\n",
        "#X_new = np.append(X_new, X[:, 9:10], axis=1)\n",
        "X_new = np.append(X_new, X[:, 12:13], axis=1)\n",
        "X_new = np.append(X_new, X[:, 19:20], axis=1)\n",
        "X_new = np.append(X_new, X[:, 20:21], axis=1)\n",
        "#X_new = np.append(X_new, X[:, 23:24], axis=1)\n",
        "X_new = np.append(X_new, X[:, 26:27], axis=1)\n",
        "X_new = np.append(X_new, X[:, 27:28], axis=1)\n",
        "X_new = np.append(X_new, X[:, 36:37], axis=1)\n",
        "X_new = np.append(X_new, X[:, 40:41], axis=1)\n",
        "X_new = np.append(X_new, X[:, 41:42], axis=1)\n",
        "X_new = np.append(X_new, X[:, 43:44], axis=1)\n",
        "X_new = np.append(X_new, X[:, 45:46], axis=1)\n",
        "X_new = np.append(X_new, X[:, 48:49], axis=1)\n",
        "X_new = np.append(X_new, X[:, 53:54], axis=1)\n",
        "X_new = np.append(X_new, X[:, 54:55], axis=1)\n",
        "#X_new = np.append(X_new, X[:, 55:56], axis=1)\n",
        "\n",
        "print(X_new.shape)"
      ],
      "execution_count": 2258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(183, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtvzZ29f-TNN"
      },
      "source": [
        "#split and normalize\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, train_size=0.8 , random_state=50)\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, train_size = 7/8 ,  random_state=50)\n",
        "\n",
        "# Standardization\n",
        "\n",
        "mean_train = X_train.mean(axis=0)\n",
        "std_train = X_train.std(axis=0)\n",
        "\n",
        "std_train[std_train == 0] = 1 \n",
        "\n",
        "\n",
        "X_train_standard = (X_train - mean_train) / std_train\n",
        "X_test_standard = (X_test - mean_train) / std_train\n",
        "X_eval_standard = (X_eval - mean_train) / std_train"
      ],
      "execution_count": 2259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24qHr8ylIi2"
      },
      "source": [
        "## Design MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDcfXAXzXOYT"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib.pyplot import imshow, imsave"
      ],
      "execution_count": 2260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKoSPELploRY",
        "outputId": "0cd3f557-b358-4f90-82b1-458530256f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MODEL_NAME = 'MLP'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"MODEL_NAME = {}, DEVICE = {}\".format(MODEL_NAME, DEVICE))"
      ],
      "execution_count": 2261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL_NAME = MLP, DEVICE = cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em2A-PXglodE",
        "outputId": "f4e572c0-8a71-4469-a9c5-46043fbcbe74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "_, feature_count = X_new.shape\n",
        "print(feature_count)\n",
        "\n",
        "class HelloMLP(nn.Module):\n",
        "    def __init__(self, input_size = feature_count ):\n",
        "        super(HelloMLP, self).__init__()\n",
        "        self.mlp = nn.Sequential(            \n",
        "            # 1st layer\n",
        "            nn.Linear(input_size, 32),                \n",
        "            nn.ReLU(),                      \n",
        "            \n",
        "            # 2nd layer\n",
        "            nn.Linear(32, 16),                \n",
        "            nn.ReLU(),\n",
        "                   \n",
        "            \n",
        "            # 3rd (output) layer\n",
        "            nn.Linear(16, 21),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y_ = self.mlp(x)     # compute \n",
        "        return y_\n",
        "\n",
        "model = HelloMLP().to(DEVICE)"
      ],
      "execution_count": 2262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCS5Yvv3lzT5"
      },
      "source": [
        "\n"
      ],
      "execution_count": 2262,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wSi-WjbmxMn"
      },
      "source": [
        "## prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jOa2bQZm1hl"
      },
      "source": [
        "#define Dataset class and make one\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).float()\n",
        "        self.target = torch.from_numpy(target).long()\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "train_dataset = MyDataset(X_train_standard, y_train)\n",
        "test_dataset = MyDataset(X_test_standard, y_test)\n",
        "eval_dataset = MyDataset(X_eval_standard, y_eval)\n"
      ],
      "execution_count": 2263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bfVWg2vwUJq"
      },
      "source": [
        "#define dataloader\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader( dataset = train_dataset, batch_size= batch_size, shuffle=True,  drop_last=True)\n",
        "eval_loader = DataLoader( dataset = eval_dataset, batch_size=100, shuffle = False,  drop_last = False)\n",
        "test_loader = DataLoader( dataset = test_dataset, batch_size= 100, shuffle=False,  drop_last = False)"
      ],
      "execution_count": 2264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qoe0Rwdm12-"
      },
      "source": [
        "## train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iks5jZTvm9Bt"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 2265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhLG9OWqm99t"
      },
      "source": [
        "# set loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# set optimizer\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 2266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlsJLbYTm_39"
      },
      "source": [
        "# reset loss history\n",
        "all_losses = []"
      ],
      "execution_count": 2267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTz427FinBhf",
        "outputId": "e8f2fd49-4ede-4e19-b51d-09be08595a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "max_epoch = 1000    # maximum number of epochs\n",
        "step = 0             # initialize step counter variable\n",
        "\n",
        "plot_every = 20\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    for idx, (x, y) in enumerate(train_loader):\n",
        "        y_hat = model(x)   \n",
        "\n",
        "        loss = loss_fn(y_hat, y)    # computing loss\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        optim.zero_grad()           # reset gradient\n",
        "        loss.backward()             # back-propagation (compute gradient)\n",
        "        optim.step()                # update parameters with gradient\n",
        "        \n",
        "        # periodically print loss\n",
        "        if step % 20 == 0:\n",
        "            print('Epoch({}): {}/{}, Step: {}, Loss: {}'.format(timeSince(start), epoch, max_epoch, step, loss.item()))\n",
        "        \n",
        "        if (step + 1) % plot_every == 0:\n",
        "            all_losses.append(total_loss / plot_every)\n",
        "            total_loss = 0\n",
        "        \n",
        "        # periodically evalute model on test data\n",
        "        if step % 20 == 0:\n",
        "            model.eval()\n",
        "            acc = 0.\n",
        "            with torch.no_grad():  \n",
        "                for idx, (x, y) in enumerate(eval_loader):\n",
        "                    y_hat = model(x) # (N, 21)\n",
        "                    loss = loss_fn(y_hat, y)\n",
        "                    _, indices = torch.max(y_hat, dim=-1)     \n",
        "                    acc += torch.sum(indices == y).item()   \n",
        "\n",
        "                    \n",
        "                    \n",
        "            print('*'*20, 'Test', '*'*20)\n",
        "            print('Step: {}, Loss: {}, test accuracy: {} %'.format(step, loss.item(), acc/len(eval_dataset)*100))\n",
        "            \n",
        "            print('*'*46)\n",
        "            model.train()           # turn to train mode (enable autograd)\n",
        "        step += 1"
      ],
      "execution_count": 2268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch(0m 0s): 0/1000, Step: 0, Loss: 3.117050886154175\n",
            "******************** Test ********************\n",
            "Step: 0, Loss: 3.007519483566284, test accuracy: 0.0 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 1/1000, Step: 20, Loss: 2.9924397468566895\n",
            "******************** Test ********************\n",
            "Step: 20, Loss: 2.954155921936035, test accuracy: 0.0 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 2/1000, Step: 40, Loss: 2.9977517127990723\n",
            "******************** Test ********************\n",
            "Step: 40, Loss: 2.888976573944092, test accuracy: 0.0 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 4/1000, Step: 60, Loss: 2.8693509101867676\n",
            "******************** Test ********************\n",
            "Step: 60, Loss: 2.808903932571411, test accuracy: 0.0 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 5/1000, Step: 80, Loss: 2.6566543579101562\n",
            "******************** Test ********************\n",
            "Step: 80, Loss: 2.734820604324341, test accuracy: 0.0 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 6/1000, Step: 100, Loss: 2.82427716255188\n",
            "******************** Test ********************\n",
            "Step: 100, Loss: 2.6444835662841797, test accuracy: 5.263157894736842 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 8/1000, Step: 120, Loss: 2.7402572631835938\n",
            "******************** Test ********************\n",
            "Step: 120, Loss: 2.5616018772125244, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 9/1000, Step: 140, Loss: 2.451967716217041\n",
            "******************** Test ********************\n",
            "Step: 140, Loss: 2.511591911315918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 10/1000, Step: 160, Loss: 2.266554594039917\n",
            "******************** Test ********************\n",
            "Step: 160, Loss: 2.4717297554016113, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 12/1000, Step: 180, Loss: 2.1087892055511475\n",
            "******************** Test ********************\n",
            "Step: 180, Loss: 2.465165376663208, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 13/1000, Step: 200, Loss: 2.1540064811706543\n",
            "******************** Test ********************\n",
            "Step: 200, Loss: 2.448260545730591, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 14/1000, Step: 220, Loss: 2.3847575187683105\n",
            "******************** Test ********************\n",
            "Step: 220, Loss: 2.4486076831817627, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 16/1000, Step: 240, Loss: 2.4863431453704834\n",
            "******************** Test ********************\n",
            "Step: 240, Loss: 2.4460179805755615, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 17/1000, Step: 260, Loss: 2.202944278717041\n",
            "******************** Test ********************\n",
            "Step: 260, Loss: 2.4757306575775146, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 18/1000, Step: 280, Loss: 1.961151123046875\n",
            "******************** Test ********************\n",
            "Step: 280, Loss: 2.5177206993103027, test accuracy: 5.263157894736842 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 20/1000, Step: 300, Loss: 1.8665978908538818\n",
            "******************** Test ********************\n",
            "Step: 300, Loss: 2.5135891437530518, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 21/1000, Step: 320, Loss: 2.2907981872558594\n",
            "******************** Test ********************\n",
            "Step: 320, Loss: 2.529144763946533, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 22/1000, Step: 340, Loss: 1.817591905593872\n",
            "******************** Test ********************\n",
            "Step: 340, Loss: 2.530287981033325, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 24/1000, Step: 360, Loss: 2.1120798587799072\n",
            "******************** Test ********************\n",
            "Step: 360, Loss: 2.5497992038726807, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 25/1000, Step: 380, Loss: 2.4672598838806152\n",
            "******************** Test ********************\n",
            "Step: 380, Loss: 2.5865180492401123, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 26/1000, Step: 400, Loss: 1.729629635810852\n",
            "******************** Test ********************\n",
            "Step: 400, Loss: 2.6098389625549316, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 28/1000, Step: 420, Loss: 1.439727544784546\n",
            "******************** Test ********************\n",
            "Step: 420, Loss: 2.6026933193206787, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 29/1000, Step: 440, Loss: 2.384551525115967\n",
            "******************** Test ********************\n",
            "Step: 440, Loss: 2.616655111312866, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 30/1000, Step: 460, Loss: 2.288713216781616\n",
            "******************** Test ********************\n",
            "Step: 460, Loss: 2.643003225326538, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 32/1000, Step: 480, Loss: 2.0203285217285156\n",
            "******************** Test ********************\n",
            "Step: 480, Loss: 2.633348226547241, test accuracy: 21.052631578947366 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 33/1000, Step: 500, Loss: 2.0331978797912598\n",
            "******************** Test ********************\n",
            "Step: 500, Loss: 2.630560874938965, test accuracy: 21.052631578947366 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 34/1000, Step: 520, Loss: 2.1296935081481934\n",
            "******************** Test ********************\n",
            "Step: 520, Loss: 2.658461570739746, test accuracy: 21.052631578947366 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 36/1000, Step: 540, Loss: 1.9338353872299194\n",
            "******************** Test ********************\n",
            "Step: 540, Loss: 2.64489483833313, test accuracy: 21.052631578947366 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 37/1000, Step: 560, Loss: 1.798243522644043\n",
            "******************** Test ********************\n",
            "Step: 560, Loss: 2.6725876331329346, test accuracy: 21.052631578947366 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 38/1000, Step: 580, Loss: 2.2498955726623535\n",
            "******************** Test ********************\n",
            "Step: 580, Loss: 2.7221639156341553, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 40/1000, Step: 600, Loss: 1.4431769847869873\n",
            "******************** Test ********************\n",
            "Step: 600, Loss: 2.7694907188415527, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 41/1000, Step: 620, Loss: 1.9849727153778076\n",
            "******************** Test ********************\n",
            "Step: 620, Loss: 2.7806015014648438, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 42/1000, Step: 640, Loss: 1.7943512201309204\n",
            "******************** Test ********************\n",
            "Step: 640, Loss: 2.7999680042266846, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 0s): 44/1000, Step: 660, Loss: 1.9442877769470215\n",
            "******************** Test ********************\n",
            "Step: 660, Loss: 2.80537486076355, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 45/1000, Step: 680, Loss: 1.5672603845596313\n",
            "******************** Test ********************\n",
            "Step: 680, Loss: 2.825941562652588, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 46/1000, Step: 700, Loss: 1.8225570917129517\n",
            "******************** Test ********************\n",
            "Step: 700, Loss: 2.865450620651245, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 48/1000, Step: 720, Loss: 1.564579963684082\n",
            "******************** Test ********************\n",
            "Step: 720, Loss: 2.85788631439209, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 49/1000, Step: 740, Loss: 1.58159339427948\n",
            "******************** Test ********************\n",
            "Step: 740, Loss: 2.902038097381592, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 50/1000, Step: 760, Loss: 1.2836016416549683\n",
            "******************** Test ********************\n",
            "Step: 760, Loss: 2.91998553276062, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 52/1000, Step: 780, Loss: 1.4065864086151123\n",
            "******************** Test ********************\n",
            "Step: 780, Loss: 2.917989730834961, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 53/1000, Step: 800, Loss: 1.3908345699310303\n",
            "******************** Test ********************\n",
            "Step: 800, Loss: 2.9748966693878174, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 54/1000, Step: 820, Loss: 1.464550256729126\n",
            "******************** Test ********************\n",
            "Step: 820, Loss: 2.975916862487793, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 56/1000, Step: 840, Loss: 1.45948326587677\n",
            "******************** Test ********************\n",
            "Step: 840, Loss: 3.0138607025146484, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 57/1000, Step: 860, Loss: 1.694370985031128\n",
            "******************** Test ********************\n",
            "Step: 860, Loss: 3.030120611190796, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 58/1000, Step: 880, Loss: 1.4182130098342896\n",
            "******************** Test ********************\n",
            "Step: 880, Loss: 3.0524771213531494, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 60/1000, Step: 900, Loss: 1.3719673156738281\n",
            "******************** Test ********************\n",
            "Step: 900, Loss: 3.1074016094207764, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 61/1000, Step: 920, Loss: 1.4087779521942139\n",
            "******************** Test ********************\n",
            "Step: 920, Loss: 3.134594202041626, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 62/1000, Step: 940, Loss: 1.1206002235412598\n",
            "******************** Test ********************\n",
            "Step: 940, Loss: 3.191542387008667, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 64/1000, Step: 960, Loss: 1.2652206420898438\n",
            "******************** Test ********************\n",
            "Step: 960, Loss: 3.2074456214904785, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 65/1000, Step: 980, Loss: 1.0030773878097534\n",
            "******************** Test ********************\n",
            "Step: 980, Loss: 3.2795963287353516, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 66/1000, Step: 1000, Loss: 1.3201943635940552\n",
            "******************** Test ********************\n",
            "Step: 1000, Loss: 3.295042037963867, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 68/1000, Step: 1020, Loss: 0.7420833110809326\n",
            "******************** Test ********************\n",
            "Step: 1020, Loss: 3.302741289138794, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 69/1000, Step: 1040, Loss: 0.9433993101119995\n",
            "******************** Test ********************\n",
            "Step: 1040, Loss: 3.2966420650482178, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 70/1000, Step: 1060, Loss: 0.881900429725647\n",
            "******************** Test ********************\n",
            "Step: 1060, Loss: 3.3509387969970703, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 72/1000, Step: 1080, Loss: 1.0986735820770264\n",
            "******************** Test ********************\n",
            "Step: 1080, Loss: 3.375338554382324, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 73/1000, Step: 1100, Loss: 1.126708984375\n",
            "******************** Test ********************\n",
            "Step: 1100, Loss: 3.488100051879883, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 74/1000, Step: 1120, Loss: 1.3421053886413574\n",
            "******************** Test ********************\n",
            "Step: 1120, Loss: 3.4734010696411133, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 76/1000, Step: 1140, Loss: 0.8372445702552795\n",
            "******************** Test ********************\n",
            "Step: 1140, Loss: 3.492158889770508, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 77/1000, Step: 1160, Loss: 1.247859239578247\n",
            "******************** Test ********************\n",
            "Step: 1160, Loss: 3.5262608528137207, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 78/1000, Step: 1180, Loss: 0.8121920228004456\n",
            "******************** Test ********************\n",
            "Step: 1180, Loss: 3.543421983718872, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 80/1000, Step: 1200, Loss: 0.7378858327865601\n",
            "******************** Test ********************\n",
            "Step: 1200, Loss: 3.6772541999816895, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 81/1000, Step: 1220, Loss: 1.0754650831222534\n",
            "******************** Test ********************\n",
            "Step: 1220, Loss: 3.6279847621917725, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 82/1000, Step: 1240, Loss: 0.8184475302696228\n",
            "******************** Test ********************\n",
            "Step: 1240, Loss: 3.719557523727417, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 84/1000, Step: 1260, Loss: 1.5833253860473633\n",
            "******************** Test ********************\n",
            "Step: 1260, Loss: 3.748414993286133, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 85/1000, Step: 1280, Loss: 0.5809130668640137\n",
            "******************** Test ********************\n",
            "Step: 1280, Loss: 3.753066301345825, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 86/1000, Step: 1300, Loss: 1.2322903871536255\n",
            "******************** Test ********************\n",
            "Step: 1300, Loss: 3.8052918910980225, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 88/1000, Step: 1320, Loss: 0.9451268911361694\n",
            "******************** Test ********************\n",
            "Step: 1320, Loss: 3.8630199432373047, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 1s): 89/1000, Step: 1340, Loss: 0.9288324117660522\n",
            "******************** Test ********************\n",
            "Step: 1340, Loss: 3.9281957149505615, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 90/1000, Step: 1360, Loss: 0.5535362362861633\n",
            "******************** Test ********************\n",
            "Step: 1360, Loss: 3.939319372177124, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 92/1000, Step: 1380, Loss: 0.7731917500495911\n",
            "******************** Test ********************\n",
            "Step: 1380, Loss: 3.933934211730957, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 93/1000, Step: 1400, Loss: 0.6413823366165161\n",
            "******************** Test ********************\n",
            "Step: 1400, Loss: 3.9952285289764404, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 94/1000, Step: 1420, Loss: 0.7890375852584839\n",
            "******************** Test ********************\n",
            "Step: 1420, Loss: 4.0111517906188965, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 96/1000, Step: 1440, Loss: 0.5608401298522949\n",
            "******************** Test ********************\n",
            "Step: 1440, Loss: 4.078423023223877, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 97/1000, Step: 1460, Loss: 0.7552862763404846\n",
            "******************** Test ********************\n",
            "Step: 1460, Loss: 4.161099433898926, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 98/1000, Step: 1480, Loss: 0.8868240714073181\n",
            "******************** Test ********************\n",
            "Step: 1480, Loss: 4.199382781982422, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 100/1000, Step: 1500, Loss: 1.1159090995788574\n",
            "******************** Test ********************\n",
            "Step: 1500, Loss: 4.220450401306152, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 101/1000, Step: 1520, Loss: 1.014629602432251\n",
            "******************** Test ********************\n",
            "Step: 1520, Loss: 4.309268951416016, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 102/1000, Step: 1540, Loss: 0.6251006126403809\n",
            "******************** Test ********************\n",
            "Step: 1540, Loss: 4.35089111328125, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 104/1000, Step: 1560, Loss: 0.35866132378578186\n",
            "******************** Test ********************\n",
            "Step: 1560, Loss: 4.31758975982666, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 105/1000, Step: 1580, Loss: 0.6169593930244446\n",
            "******************** Test ********************\n",
            "Step: 1580, Loss: 4.446840763092041, test accuracy: 10.526315789473683 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 106/1000, Step: 1600, Loss: 0.810946524143219\n",
            "******************** Test ********************\n",
            "Step: 1600, Loss: 4.487119674682617, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 108/1000, Step: 1620, Loss: 0.5366339683532715\n",
            "******************** Test ********************\n",
            "Step: 1620, Loss: 4.4767632484436035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 109/1000, Step: 1640, Loss: 0.6890329718589783\n",
            "******************** Test ********************\n",
            "Step: 1640, Loss: 4.520328044891357, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 110/1000, Step: 1660, Loss: 0.7607116103172302\n",
            "******************** Test ********************\n",
            "Step: 1660, Loss: 4.536567687988281, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 112/1000, Step: 1680, Loss: 0.8284332752227783\n",
            "******************** Test ********************\n",
            "Step: 1680, Loss: 4.700838565826416, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 113/1000, Step: 1700, Loss: 0.7397081255912781\n",
            "******************** Test ********************\n",
            "Step: 1700, Loss: 4.671318054199219, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 114/1000, Step: 1720, Loss: 0.29223254323005676\n",
            "******************** Test ********************\n",
            "Step: 1720, Loss: 4.658921718597412, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 116/1000, Step: 1740, Loss: 0.5462033748626709\n",
            "******************** Test ********************\n",
            "Step: 1740, Loss: 4.776298522949219, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 117/1000, Step: 1760, Loss: 0.5718463659286499\n",
            "******************** Test ********************\n",
            "Step: 1760, Loss: 4.789607048034668, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 118/1000, Step: 1780, Loss: 0.6730128526687622\n",
            "******************** Test ********************\n",
            "Step: 1780, Loss: 4.8968095779418945, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 120/1000, Step: 1800, Loss: 0.6228043437004089\n",
            "******************** Test ********************\n",
            "Step: 1800, Loss: 4.867323875427246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 121/1000, Step: 1820, Loss: 0.7719463109970093\n",
            "******************** Test ********************\n",
            "Step: 1820, Loss: 4.967463493347168, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 122/1000, Step: 1840, Loss: 0.6393336653709412\n",
            "******************** Test ********************\n",
            "Step: 1840, Loss: 5.076915264129639, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 124/1000, Step: 1860, Loss: 0.4148966670036316\n",
            "******************** Test ********************\n",
            "Step: 1860, Loss: 5.045964241027832, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 125/1000, Step: 1880, Loss: 0.6005970239639282\n",
            "******************** Test ********************\n",
            "Step: 1880, Loss: 5.068362712860107, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 126/1000, Step: 1900, Loss: 0.27782267332077026\n",
            "******************** Test ********************\n",
            "Step: 1900, Loss: 5.155996799468994, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 128/1000, Step: 1920, Loss: 0.36422187089920044\n",
            "******************** Test ********************\n",
            "Step: 1920, Loss: 5.238107204437256, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 129/1000, Step: 1940, Loss: 0.7503684163093567\n",
            "******************** Test ********************\n",
            "Step: 1940, Loss: 5.264618396759033, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 130/1000, Step: 1960, Loss: 0.6457117795944214\n",
            "******************** Test ********************\n",
            "Step: 1960, Loss: 5.2647905349731445, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 2s): 132/1000, Step: 1980, Loss: 0.11572171747684479\n",
            "******************** Test ********************\n",
            "Step: 1980, Loss: 5.363137722015381, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 133/1000, Step: 2000, Loss: 0.48463132977485657\n",
            "******************** Test ********************\n",
            "Step: 2000, Loss: 5.410820484161377, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 134/1000, Step: 2020, Loss: 0.45726823806762695\n",
            "******************** Test ********************\n",
            "Step: 2020, Loss: 5.476710319519043, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 136/1000, Step: 2040, Loss: 0.3121231198310852\n",
            "******************** Test ********************\n",
            "Step: 2040, Loss: 5.486544132232666, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 137/1000, Step: 2060, Loss: 0.3277144134044647\n",
            "******************** Test ********************\n",
            "Step: 2060, Loss: 5.627601146697998, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 138/1000, Step: 2080, Loss: 0.358271062374115\n",
            "******************** Test ********************\n",
            "Step: 2080, Loss: 5.625702381134033, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 140/1000, Step: 2100, Loss: 0.39070746302604675\n",
            "******************** Test ********************\n",
            "Step: 2100, Loss: 5.657166957855225, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 141/1000, Step: 2120, Loss: 0.2201836109161377\n",
            "******************** Test ********************\n",
            "Step: 2120, Loss: 5.743427276611328, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 142/1000, Step: 2140, Loss: 0.5656347870826721\n",
            "******************** Test ********************\n",
            "Step: 2140, Loss: 5.799500942230225, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 144/1000, Step: 2160, Loss: 0.32578474283218384\n",
            "******************** Test ********************\n",
            "Step: 2160, Loss: 5.85508394241333, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 145/1000, Step: 2180, Loss: 0.09298759698867798\n",
            "******************** Test ********************\n",
            "Step: 2180, Loss: 5.874520301818848, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 146/1000, Step: 2200, Loss: 0.6371147632598877\n",
            "******************** Test ********************\n",
            "Step: 2200, Loss: 5.950558662414551, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 148/1000, Step: 2220, Loss: 0.18239018321037292\n",
            "******************** Test ********************\n",
            "Step: 2220, Loss: 5.9245405197143555, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 149/1000, Step: 2240, Loss: 0.16944891214370728\n",
            "******************** Test ********************\n",
            "Step: 2240, Loss: 5.981367588043213, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 150/1000, Step: 2260, Loss: 0.44326430559158325\n",
            "******************** Test ********************\n",
            "Step: 2260, Loss: 6.117616176605225, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 152/1000, Step: 2280, Loss: 0.27893951535224915\n",
            "******************** Test ********************\n",
            "Step: 2280, Loss: 6.1204729080200195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 153/1000, Step: 2300, Loss: 0.38393786549568176\n",
            "******************** Test ********************\n",
            "Step: 2300, Loss: 6.189856052398682, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 154/1000, Step: 2320, Loss: 0.33482348918914795\n",
            "******************** Test ********************\n",
            "Step: 2320, Loss: 6.1640944480896, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 156/1000, Step: 2340, Loss: 0.3304210305213928\n",
            "******************** Test ********************\n",
            "Step: 2340, Loss: 6.210485458374023, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 157/1000, Step: 2360, Loss: 0.2741699516773224\n",
            "******************** Test ********************\n",
            "Step: 2360, Loss: 6.35486364364624, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 158/1000, Step: 2380, Loss: 0.23714463412761688\n",
            "******************** Test ********************\n",
            "Step: 2380, Loss: 6.309414386749268, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 160/1000, Step: 2400, Loss: 0.40573811531066895\n",
            "******************** Test ********************\n",
            "Step: 2400, Loss: 6.332977294921875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 161/1000, Step: 2420, Loss: 0.39927950501441956\n",
            "******************** Test ********************\n",
            "Step: 2420, Loss: 6.444303035736084, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 162/1000, Step: 2440, Loss: 0.7666332721710205\n",
            "******************** Test ********************\n",
            "Step: 2440, Loss: 6.432300567626953, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 164/1000, Step: 2460, Loss: 0.2453419715166092\n",
            "******************** Test ********************\n",
            "Step: 2460, Loss: 6.516002655029297, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 165/1000, Step: 2480, Loss: 0.2812560200691223\n",
            "******************** Test ********************\n",
            "Step: 2480, Loss: 6.584672451019287, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 166/1000, Step: 2500, Loss: 0.36039334535598755\n",
            "******************** Test ********************\n",
            "Step: 2500, Loss: 6.646432399749756, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 168/1000, Step: 2520, Loss: 0.3133651912212372\n",
            "******************** Test ********************\n",
            "Step: 2520, Loss: 6.6667680740356445, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 169/1000, Step: 2540, Loss: 0.3953694701194763\n",
            "******************** Test ********************\n",
            "Step: 2540, Loss: 6.718288421630859, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 170/1000, Step: 2560, Loss: 0.3438928723335266\n",
            "******************** Test ********************\n",
            "Step: 2560, Loss: 6.8513054847717285, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 172/1000, Step: 2580, Loss: 0.0966382697224617\n",
            "******************** Test ********************\n",
            "Step: 2580, Loss: 6.868433952331543, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 173/1000, Step: 2600, Loss: 0.14052124321460724\n",
            "******************** Test ********************\n",
            "Step: 2600, Loss: 6.8601226806640625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 174/1000, Step: 2620, Loss: 0.49459248781204224\n",
            "******************** Test ********************\n",
            "Step: 2620, Loss: 6.934438705444336, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 3s): 176/1000, Step: 2640, Loss: 0.10052873194217682\n",
            "******************** Test ********************\n",
            "Step: 2640, Loss: 6.968557357788086, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 177/1000, Step: 2660, Loss: 0.18707793951034546\n",
            "******************** Test ********************\n",
            "Step: 2660, Loss: 6.935251235961914, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 178/1000, Step: 2680, Loss: 0.27693408727645874\n",
            "******************** Test ********************\n",
            "Step: 2680, Loss: 6.975541114807129, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 180/1000, Step: 2700, Loss: 0.18592528998851776\n",
            "******************** Test ********************\n",
            "Step: 2700, Loss: 7.126582145690918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 181/1000, Step: 2720, Loss: 0.09625238180160522\n",
            "******************** Test ********************\n",
            "Step: 2720, Loss: 7.1318559646606445, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 182/1000, Step: 2740, Loss: 0.19275633990764618\n",
            "******************** Test ********************\n",
            "Step: 2740, Loss: 7.214838981628418, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 184/1000, Step: 2760, Loss: 0.24982047080993652\n",
            "******************** Test ********************\n",
            "Step: 2760, Loss: 7.251056671142578, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 185/1000, Step: 2780, Loss: 0.34543025493621826\n",
            "******************** Test ********************\n",
            "Step: 2780, Loss: 7.3067402839660645, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 186/1000, Step: 2800, Loss: 0.23173099756240845\n",
            "******************** Test ********************\n",
            "Step: 2800, Loss: 7.364192962646484, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 188/1000, Step: 2820, Loss: 0.23887605965137482\n",
            "******************** Test ********************\n",
            "Step: 2820, Loss: 7.333864688873291, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 189/1000, Step: 2840, Loss: 0.2628418207168579\n",
            "******************** Test ********************\n",
            "Step: 2840, Loss: 7.346435546875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 190/1000, Step: 2860, Loss: 0.1703346222639084\n",
            "******************** Test ********************\n",
            "Step: 2860, Loss: 7.485910415649414, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 192/1000, Step: 2880, Loss: 0.3291299045085907\n",
            "******************** Test ********************\n",
            "Step: 2880, Loss: 7.546487808227539, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 193/1000, Step: 2900, Loss: 0.41624394059181213\n",
            "******************** Test ********************\n",
            "Step: 2900, Loss: 7.571817398071289, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 194/1000, Step: 2920, Loss: 0.6908656358718872\n",
            "******************** Test ********************\n",
            "Step: 2920, Loss: 7.640527725219727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 196/1000, Step: 2940, Loss: 0.08954071998596191\n",
            "******************** Test ********************\n",
            "Step: 2940, Loss: 7.7424235343933105, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 197/1000, Step: 2960, Loss: 0.19282010197639465\n",
            "******************** Test ********************\n",
            "Step: 2960, Loss: 7.687135219573975, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 198/1000, Step: 2980, Loss: 0.23467481136322021\n",
            "******************** Test ********************\n",
            "Step: 2980, Loss: 7.795901775360107, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 200/1000, Step: 3000, Loss: 0.1609390825033188\n",
            "******************** Test ********************\n",
            "Step: 3000, Loss: 7.8127899169921875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 201/1000, Step: 3020, Loss: 0.08125636726617813\n",
            "******************** Test ********************\n",
            "Step: 3020, Loss: 7.8457183837890625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 202/1000, Step: 3040, Loss: 0.07975921034812927\n",
            "******************** Test ********************\n",
            "Step: 3040, Loss: 7.822871208190918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 204/1000, Step: 3060, Loss: 0.13916683197021484\n",
            "******************** Test ********************\n",
            "Step: 3060, Loss: 7.95556640625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 205/1000, Step: 3080, Loss: 0.12093257158994675\n",
            "******************** Test ********************\n",
            "Step: 3080, Loss: 7.985055923461914, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 206/1000, Step: 3100, Loss: 0.24849095940589905\n",
            "******************** Test ********************\n",
            "Step: 3100, Loss: 7.9755353927612305, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 208/1000, Step: 3120, Loss: 0.15343157947063446\n",
            "******************** Test ********************\n",
            "Step: 3120, Loss: 8.017231941223145, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 209/1000, Step: 3140, Loss: 0.2486264705657959\n",
            "******************** Test ********************\n",
            "Step: 3140, Loss: 8.156898498535156, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 210/1000, Step: 3160, Loss: 0.17357660830020905\n",
            "******************** Test ********************\n",
            "Step: 3160, Loss: 8.172846794128418, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 212/1000, Step: 3180, Loss: 0.1739831119775772\n",
            "******************** Test ********************\n",
            "Step: 3180, Loss: 8.123329162597656, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 213/1000, Step: 3200, Loss: 0.06203167885541916\n",
            "******************** Test ********************\n",
            "Step: 3200, Loss: 8.304043769836426, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 214/1000, Step: 3220, Loss: 0.1964140385389328\n",
            "******************** Test ********************\n",
            "Step: 3220, Loss: 8.242461204528809, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 216/1000, Step: 3240, Loss: 0.25732341408729553\n",
            "******************** Test ********************\n",
            "Step: 3240, Loss: 8.359678268432617, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 217/1000, Step: 3260, Loss: 0.07639888674020767\n",
            "******************** Test ********************\n",
            "Step: 3260, Loss: 8.405285835266113, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 218/1000, Step: 3280, Loss: 0.20880571007728577\n",
            "******************** Test ********************\n",
            "Step: 3280, Loss: 8.452935218811035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 220/1000, Step: 3300, Loss: 0.22365854680538177\n",
            "******************** Test ********************\n",
            "Step: 3300, Loss: 8.439085006713867, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 4s): 221/1000, Step: 3320, Loss: 0.04258844256401062\n",
            "******************** Test ********************\n",
            "Step: 3320, Loss: 8.508979797363281, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 222/1000, Step: 3340, Loss: 0.08297048509120941\n",
            "******************** Test ********************\n",
            "Step: 3340, Loss: 8.629288673400879, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 224/1000, Step: 3360, Loss: 0.05891913175582886\n",
            "******************** Test ********************\n",
            "Step: 3360, Loss: 8.6441650390625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 225/1000, Step: 3380, Loss: 0.2854812741279602\n",
            "******************** Test ********************\n",
            "Step: 3380, Loss: 8.630105972290039, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 226/1000, Step: 3400, Loss: 0.06690725684165955\n",
            "******************** Test ********************\n",
            "Step: 3400, Loss: 8.640497207641602, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 228/1000, Step: 3420, Loss: 0.11537302285432816\n",
            "******************** Test ********************\n",
            "Step: 3420, Loss: 8.687223434448242, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 229/1000, Step: 3440, Loss: 0.12169970571994781\n",
            "******************** Test ********************\n",
            "Step: 3440, Loss: 8.79520320892334, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 230/1000, Step: 3460, Loss: 0.24236540496349335\n",
            "******************** Test ********************\n",
            "Step: 3460, Loss: 8.784834861755371, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 232/1000, Step: 3480, Loss: 0.2476203590631485\n",
            "******************** Test ********************\n",
            "Step: 3480, Loss: 8.825576782226562, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 233/1000, Step: 3500, Loss: 0.2206905037164688\n",
            "******************** Test ********************\n",
            "Step: 3500, Loss: 8.887133598327637, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 234/1000, Step: 3520, Loss: 0.06473347544670105\n",
            "******************** Test ********************\n",
            "Step: 3520, Loss: 8.910778999328613, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 236/1000, Step: 3540, Loss: 0.07342329621315002\n",
            "******************** Test ********************\n",
            "Step: 3540, Loss: 8.945534706115723, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 237/1000, Step: 3560, Loss: 0.25587770342826843\n",
            "******************** Test ********************\n",
            "Step: 3560, Loss: 9.005021095275879, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 238/1000, Step: 3580, Loss: 0.06303870677947998\n",
            "******************** Test ********************\n",
            "Step: 3580, Loss: 9.051068305969238, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 240/1000, Step: 3600, Loss: 0.058654189109802246\n",
            "******************** Test ********************\n",
            "Step: 3600, Loss: 9.182538032531738, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 241/1000, Step: 3620, Loss: 0.11695732176303864\n",
            "******************** Test ********************\n",
            "Step: 3620, Loss: 9.145009994506836, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 242/1000, Step: 3640, Loss: 0.19230982661247253\n",
            "******************** Test ********************\n",
            "Step: 3640, Loss: 9.200627326965332, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 244/1000, Step: 3660, Loss: 0.25265705585479736\n",
            "******************** Test ********************\n",
            "Step: 3660, Loss: 9.212657928466797, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 245/1000, Step: 3680, Loss: 0.05971847102046013\n",
            "******************** Test ********************\n",
            "Step: 3680, Loss: 9.274284362792969, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 246/1000, Step: 3700, Loss: 0.32164841890335083\n",
            "******************** Test ********************\n",
            "Step: 3700, Loss: 9.275969505310059, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 248/1000, Step: 3720, Loss: 0.17738717794418335\n",
            "******************** Test ********************\n",
            "Step: 3720, Loss: 9.330986022949219, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 249/1000, Step: 3740, Loss: 0.04894633591175079\n",
            "******************** Test ********************\n",
            "Step: 3740, Loss: 9.367459297180176, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 250/1000, Step: 3760, Loss: 0.02402692846953869\n",
            "******************** Test ********************\n",
            "Step: 3760, Loss: 9.368037223815918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 252/1000, Step: 3780, Loss: 0.18180803954601288\n",
            "******************** Test ********************\n",
            "Step: 3780, Loss: 9.456254005432129, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 253/1000, Step: 3800, Loss: 0.17075622081756592\n",
            "******************** Test ********************\n",
            "Step: 3800, Loss: 9.47004222869873, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 254/1000, Step: 3820, Loss: 0.20548206567764282\n",
            "******************** Test ********************\n",
            "Step: 3820, Loss: 9.507719993591309, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 256/1000, Step: 3840, Loss: 0.15345469117164612\n",
            "******************** Test ********************\n",
            "Step: 3840, Loss: 9.622762680053711, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 257/1000, Step: 3860, Loss: 0.11895248293876648\n",
            "******************** Test ********************\n",
            "Step: 3860, Loss: 9.676480293273926, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 258/1000, Step: 3880, Loss: 0.07219502329826355\n",
            "******************** Test ********************\n",
            "Step: 3880, Loss: 9.669181823730469, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 260/1000, Step: 3900, Loss: 0.02917742170393467\n",
            "******************** Test ********************\n",
            "Step: 3900, Loss: 9.722310066223145, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 261/1000, Step: 3920, Loss: 0.0722624808549881\n",
            "******************** Test ********************\n",
            "Step: 3920, Loss: 9.785698890686035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 262/1000, Step: 3940, Loss: 0.035274334251880646\n",
            "******************** Test ********************\n",
            "Step: 3940, Loss: 9.797649383544922, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 264/1000, Step: 3960, Loss: 0.0790497288107872\n",
            "******************** Test ********************\n",
            "Step: 3960, Loss: 9.836771965026855, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 265/1000, Step: 3980, Loss: 0.06955275684595108\n",
            "******************** Test ********************\n",
            "Step: 3980, Loss: 9.84941577911377, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 266/1000, Step: 4000, Loss: 0.035298410803079605\n",
            "******************** Test ********************\n",
            "Step: 4000, Loss: 9.866081237792969, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 5s): 268/1000, Step: 4020, Loss: 0.033940620720386505\n",
            "******************** Test ********************\n",
            "Step: 4020, Loss: 10.035367965698242, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 269/1000, Step: 4040, Loss: 0.04048503562808037\n",
            "******************** Test ********************\n",
            "Step: 4040, Loss: 10.102014541625977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 270/1000, Step: 4060, Loss: 0.15732994675636292\n",
            "******************** Test ********************\n",
            "Step: 4060, Loss: 10.047014236450195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 272/1000, Step: 4080, Loss: 0.06848181784152985\n",
            "******************** Test ********************\n",
            "Step: 4080, Loss: 10.100908279418945, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 273/1000, Step: 4100, Loss: 0.16916562616825104\n",
            "******************** Test ********************\n",
            "Step: 4100, Loss: 10.12735652923584, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 274/1000, Step: 4120, Loss: 0.09161204844713211\n",
            "******************** Test ********************\n",
            "Step: 4120, Loss: 10.180717468261719, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 276/1000, Step: 4140, Loss: 0.0709247961640358\n",
            "******************** Test ********************\n",
            "Step: 4140, Loss: 10.258866310119629, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 277/1000, Step: 4160, Loss: 0.13862715661525726\n",
            "******************** Test ********************\n",
            "Step: 4160, Loss: 10.178295135498047, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 278/1000, Step: 4180, Loss: 0.028949828818440437\n",
            "******************** Test ********************\n",
            "Step: 4180, Loss: 10.294087409973145, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 280/1000, Step: 4200, Loss: 0.027061959728598595\n",
            "******************** Test ********************\n",
            "Step: 4200, Loss: 10.340920448303223, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 281/1000, Step: 4220, Loss: 0.024902842938899994\n",
            "******************** Test ********************\n",
            "Step: 4220, Loss: 10.402634620666504, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 282/1000, Step: 4240, Loss: 0.04044046625494957\n",
            "******************** Test ********************\n",
            "Step: 4240, Loss: 10.414040565490723, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 284/1000, Step: 4260, Loss: 0.07659071683883667\n",
            "******************** Test ********************\n",
            "Step: 4260, Loss: 10.433075904846191, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 285/1000, Step: 4280, Loss: 0.029715681448578835\n",
            "******************** Test ********************\n",
            "Step: 4280, Loss: 10.460477828979492, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 286/1000, Step: 4300, Loss: 0.03867800161242485\n",
            "******************** Test ********************\n",
            "Step: 4300, Loss: 10.5292329788208, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 288/1000, Step: 4320, Loss: 0.06145903840661049\n",
            "******************** Test ********************\n",
            "Step: 4320, Loss: 10.600584030151367, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 289/1000, Step: 4340, Loss: 0.06403937935829163\n",
            "******************** Test ********************\n",
            "Step: 4340, Loss: 10.626712799072266, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 290/1000, Step: 4360, Loss: 0.03877164050936699\n",
            "******************** Test ********************\n",
            "Step: 4360, Loss: 10.680488586425781, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 292/1000, Step: 4380, Loss: 0.06176586449146271\n",
            "******************** Test ********************\n",
            "Step: 4380, Loss: 10.69176959991455, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 293/1000, Step: 4400, Loss: 0.030196882784366608\n",
            "******************** Test ********************\n",
            "Step: 4400, Loss: 10.76017951965332, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 294/1000, Step: 4420, Loss: 0.03253323212265968\n",
            "******************** Test ********************\n",
            "Step: 4420, Loss: 10.802618026733398, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 296/1000, Step: 4440, Loss: 0.019720057025551796\n",
            "******************** Test ********************\n",
            "Step: 4440, Loss: 10.793922424316406, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 297/1000, Step: 4460, Loss: 0.03274894505739212\n",
            "******************** Test ********************\n",
            "Step: 4460, Loss: 10.866865158081055, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 298/1000, Step: 4480, Loss: 0.06616726517677307\n",
            "******************** Test ********************\n",
            "Step: 4480, Loss: 10.812200546264648, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 300/1000, Step: 4500, Loss: 0.0669640451669693\n",
            "******************** Test ********************\n",
            "Step: 4500, Loss: 10.933439254760742, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 301/1000, Step: 4520, Loss: 0.12279245257377625\n",
            "******************** Test ********************\n",
            "Step: 4520, Loss: 10.957019805908203, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 302/1000, Step: 4540, Loss: 0.14678744971752167\n",
            "******************** Test ********************\n",
            "Step: 4540, Loss: 11.025004386901855, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 304/1000, Step: 4560, Loss: 0.16992047429084778\n",
            "******************** Test ********************\n",
            "Step: 4560, Loss: 11.040326118469238, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 305/1000, Step: 4580, Loss: 0.05683952569961548\n",
            "******************** Test ********************\n",
            "Step: 4580, Loss: 11.105767250061035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 306/1000, Step: 4600, Loss: 0.0753297209739685\n",
            "******************** Test ********************\n",
            "Step: 4600, Loss: 11.052444458007812, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 308/1000, Step: 4620, Loss: 0.08414498716592789\n",
            "******************** Test ********************\n",
            "Step: 4620, Loss: 11.141544342041016, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 309/1000, Step: 4640, Loss: 0.032672736793756485\n",
            "******************** Test ********************\n",
            "Step: 4640, Loss: 11.157787322998047, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 310/1000, Step: 4660, Loss: 0.033050112426280975\n",
            "******************** Test ********************\n",
            "Step: 4660, Loss: 11.188179016113281, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 312/1000, Step: 4680, Loss: 0.1091211810708046\n",
            "******************** Test ********************\n",
            "Step: 4680, Loss: 11.264413833618164, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 313/1000, Step: 4700, Loss: 0.17701871693134308\n",
            "******************** Test ********************\n",
            "Step: 4700, Loss: 11.315858840942383, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 6s): 314/1000, Step: 4720, Loss: 0.0201139897108078\n",
            "******************** Test ********************\n",
            "Step: 4720, Loss: 11.289074897766113, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 316/1000, Step: 4740, Loss: 0.0063143218867480755\n",
            "******************** Test ********************\n",
            "Step: 4740, Loss: 11.357678413391113, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 317/1000, Step: 4760, Loss: 0.059896595776081085\n",
            "******************** Test ********************\n",
            "Step: 4760, Loss: 11.397711753845215, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 318/1000, Step: 4780, Loss: 0.10647755861282349\n",
            "******************** Test ********************\n",
            "Step: 4780, Loss: 11.379968643188477, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 320/1000, Step: 4800, Loss: 0.020669307559728622\n",
            "******************** Test ********************\n",
            "Step: 4800, Loss: 11.505794525146484, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 321/1000, Step: 4820, Loss: 0.024550123140215874\n",
            "******************** Test ********************\n",
            "Step: 4820, Loss: 11.493783950805664, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 322/1000, Step: 4840, Loss: 0.06910066306591034\n",
            "******************** Test ********************\n",
            "Step: 4840, Loss: 11.515704154968262, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 324/1000, Step: 4860, Loss: 0.040410879999399185\n",
            "******************** Test ********************\n",
            "Step: 4860, Loss: 11.565885543823242, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 325/1000, Step: 4880, Loss: 0.034638237208127975\n",
            "******************** Test ********************\n",
            "Step: 4880, Loss: 11.659234046936035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 326/1000, Step: 4900, Loss: 0.037365417927503586\n",
            "******************** Test ********************\n",
            "Step: 4900, Loss: 11.671443939208984, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 328/1000, Step: 4920, Loss: 0.010180935263633728\n",
            "******************** Test ********************\n",
            "Step: 4920, Loss: 11.594487190246582, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 329/1000, Step: 4940, Loss: 0.09213493764400482\n",
            "******************** Test ********************\n",
            "Step: 4940, Loss: 11.709196090698242, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 330/1000, Step: 4960, Loss: 0.06283854693174362\n",
            "******************** Test ********************\n",
            "Step: 4960, Loss: 11.693382263183594, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 332/1000, Step: 4980, Loss: 0.024210264906287193\n",
            "******************** Test ********************\n",
            "Step: 4980, Loss: 11.727666854858398, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 333/1000, Step: 5000, Loss: 0.18830029666423798\n",
            "******************** Test ********************\n",
            "Step: 5000, Loss: 11.785122871398926, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 334/1000, Step: 5020, Loss: 0.05384226143360138\n",
            "******************** Test ********************\n",
            "Step: 5020, Loss: 11.789389610290527, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 336/1000, Step: 5040, Loss: 0.07644806802272797\n",
            "******************** Test ********************\n",
            "Step: 5040, Loss: 11.892816543579102, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 337/1000, Step: 5060, Loss: 0.10332797467708588\n",
            "******************** Test ********************\n",
            "Step: 5060, Loss: 11.866680145263672, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 338/1000, Step: 5080, Loss: 0.14421135187149048\n",
            "******************** Test ********************\n",
            "Step: 5080, Loss: 11.921175003051758, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 340/1000, Step: 5100, Loss: 0.016955386847257614\n",
            "******************** Test ********************\n",
            "Step: 5100, Loss: 11.961657524108887, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 341/1000, Step: 5120, Loss: 0.00746082654222846\n",
            "******************** Test ********************\n",
            "Step: 5120, Loss: 11.98071002960205, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 342/1000, Step: 5140, Loss: 0.020366696640849113\n",
            "******************** Test ********************\n",
            "Step: 5140, Loss: 11.989785194396973, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 344/1000, Step: 5160, Loss: 0.011670178733766079\n",
            "******************** Test ********************\n",
            "Step: 5160, Loss: 12.1062593460083, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 345/1000, Step: 5180, Loss: 0.09483179450035095\n",
            "******************** Test ********************\n",
            "Step: 5180, Loss: 12.097551345825195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 346/1000, Step: 5200, Loss: 0.10232675820589066\n",
            "******************** Test ********************\n",
            "Step: 5200, Loss: 12.162636756896973, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 348/1000, Step: 5220, Loss: 0.08881691843271255\n",
            "******************** Test ********************\n",
            "Step: 5220, Loss: 12.21852970123291, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 349/1000, Step: 5240, Loss: 0.024356167763471603\n",
            "******************** Test ********************\n",
            "Step: 5240, Loss: 12.23941707611084, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 350/1000, Step: 5260, Loss: 0.04143188148736954\n",
            "******************** Test ********************\n",
            "Step: 5260, Loss: 12.302903175354004, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 352/1000, Step: 5280, Loss: 0.033481400460004807\n",
            "******************** Test ********************\n",
            "Step: 5280, Loss: 12.3543062210083, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 353/1000, Step: 5300, Loss: 0.006412283983081579\n",
            "******************** Test ********************\n",
            "Step: 5300, Loss: 12.362507820129395, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 354/1000, Step: 5320, Loss: 0.031408440321683884\n",
            "******************** Test ********************\n",
            "Step: 5320, Loss: 12.433009147644043, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 356/1000, Step: 5340, Loss: 0.01963682658970356\n",
            "******************** Test ********************\n",
            "Step: 5340, Loss: 12.444437026977539, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 357/1000, Step: 5360, Loss: 0.03618403524160385\n",
            "******************** Test ********************\n",
            "Step: 5360, Loss: 12.49989128112793, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 358/1000, Step: 5380, Loss: 0.029981764033436775\n",
            "******************** Test ********************\n",
            "Step: 5380, Loss: 12.437512397766113, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 360/1000, Step: 5400, Loss: 0.014099919237196445\n",
            "******************** Test ********************\n",
            "Step: 5400, Loss: 12.5665864944458, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 7s): 361/1000, Step: 5420, Loss: 0.08576683700084686\n",
            "******************** Test ********************\n",
            "Step: 5420, Loss: 12.592491149902344, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 362/1000, Step: 5440, Loss: 0.10817193239927292\n",
            "******************** Test ********************\n",
            "Step: 5440, Loss: 12.6488037109375, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 364/1000, Step: 5460, Loss: 0.015590292401611805\n",
            "******************** Test ********************\n",
            "Step: 5460, Loss: 12.707584381103516, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 365/1000, Step: 5480, Loss: 0.08659342676401138\n",
            "******************** Test ********************\n",
            "Step: 5480, Loss: 12.680859565734863, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 366/1000, Step: 5500, Loss: 0.016158130019903183\n",
            "******************** Test ********************\n",
            "Step: 5500, Loss: 12.71819019317627, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 368/1000, Step: 5520, Loss: 0.016682114452123642\n",
            "******************** Test ********************\n",
            "Step: 5520, Loss: 12.742693901062012, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 369/1000, Step: 5540, Loss: 0.09969533979892731\n",
            "******************** Test ********************\n",
            "Step: 5540, Loss: 12.786992073059082, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 370/1000, Step: 5560, Loss: 0.015730615705251694\n",
            "******************** Test ********************\n",
            "Step: 5560, Loss: 12.87356948852539, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 372/1000, Step: 5580, Loss: 0.012957613915205002\n",
            "******************** Test ********************\n",
            "Step: 5580, Loss: 12.884533882141113, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 373/1000, Step: 5600, Loss: 0.04534522816538811\n",
            "******************** Test ********************\n",
            "Step: 5600, Loss: 12.944053649902344, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 374/1000, Step: 5620, Loss: 0.011767875403165817\n",
            "******************** Test ********************\n",
            "Step: 5620, Loss: 12.948651313781738, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 376/1000, Step: 5640, Loss: 0.03460800647735596\n",
            "******************** Test ********************\n",
            "Step: 5640, Loss: 13.017289161682129, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 377/1000, Step: 5660, Loss: 0.052842456847429276\n",
            "******************** Test ********************\n",
            "Step: 5660, Loss: 13.027514457702637, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 378/1000, Step: 5680, Loss: 0.01816023886203766\n",
            "******************** Test ********************\n",
            "Step: 5680, Loss: 13.1150541305542, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 380/1000, Step: 5700, Loss: 0.028257226571440697\n",
            "******************** Test ********************\n",
            "Step: 5700, Loss: 13.146531105041504, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 381/1000, Step: 5720, Loss: 0.0565522126853466\n",
            "******************** Test ********************\n",
            "Step: 5720, Loss: 13.120699882507324, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 382/1000, Step: 5740, Loss: 0.059011511504650116\n",
            "******************** Test ********************\n",
            "Step: 5740, Loss: 13.168554306030273, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 384/1000, Step: 5760, Loss: 0.043438371270895004\n",
            "******************** Test ********************\n",
            "Step: 5760, Loss: 13.235725402832031, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 385/1000, Step: 5780, Loss: 0.012412471696734428\n",
            "******************** Test ********************\n",
            "Step: 5780, Loss: 13.322894096374512, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 386/1000, Step: 5800, Loss: 0.10627506673336029\n",
            "******************** Test ********************\n",
            "Step: 5800, Loss: 13.325447082519531, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 388/1000, Step: 5820, Loss: 0.007728182710707188\n",
            "******************** Test ********************\n",
            "Step: 5820, Loss: 13.394309997558594, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 389/1000, Step: 5840, Loss: 0.004537232220172882\n",
            "******************** Test ********************\n",
            "Step: 5840, Loss: 13.450691223144531, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 390/1000, Step: 5860, Loss: 0.05636685714125633\n",
            "******************** Test ********************\n",
            "Step: 5860, Loss: 13.443090438842773, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 392/1000, Step: 5880, Loss: 0.011156666092574596\n",
            "******************** Test ********************\n",
            "Step: 5880, Loss: 13.568455696105957, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 393/1000, Step: 5900, Loss: 0.02144794538617134\n",
            "******************** Test ********************\n",
            "Step: 5900, Loss: 13.535883903503418, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 394/1000, Step: 5920, Loss: 0.017950579524040222\n",
            "******************** Test ********************\n",
            "Step: 5920, Loss: 13.607007026672363, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 396/1000, Step: 5940, Loss: 0.011898013763129711\n",
            "******************** Test ********************\n",
            "Step: 5940, Loss: 13.591975212097168, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 397/1000, Step: 5960, Loss: 0.03468857705593109\n",
            "******************** Test ********************\n",
            "Step: 5960, Loss: 13.629770278930664, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 398/1000, Step: 5980, Loss: 0.01922057941555977\n",
            "******************** Test ********************\n",
            "Step: 5980, Loss: 13.649296760559082, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 400/1000, Step: 6000, Loss: 0.07762980461120605\n",
            "******************** Test ********************\n",
            "Step: 6000, Loss: 13.681319236755371, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 401/1000, Step: 6020, Loss: 0.08223555237054825\n",
            "******************** Test ********************\n",
            "Step: 6020, Loss: 13.711287498474121, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 402/1000, Step: 6040, Loss: 0.007156768348067999\n",
            "******************** Test ********************\n",
            "Step: 6040, Loss: 13.7501802444458, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 404/1000, Step: 6060, Loss: 0.010202532634139061\n",
            "******************** Test ********************\n",
            "Step: 6060, Loss: 13.813523292541504, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 405/1000, Step: 6080, Loss: 0.005233442410826683\n",
            "******************** Test ********************\n",
            "Step: 6080, Loss: 13.933055877685547, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 406/1000, Step: 6100, Loss: 0.013288690708577633\n",
            "******************** Test ********************\n",
            "Step: 6100, Loss: 13.895276069641113, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 8s): 408/1000, Step: 6120, Loss: 0.023336566984653473\n",
            "******************** Test ********************\n",
            "Step: 6120, Loss: 13.92034912109375, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 409/1000, Step: 6140, Loss: 0.03493117541074753\n",
            "******************** Test ********************\n",
            "Step: 6140, Loss: 13.961705207824707, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 410/1000, Step: 6160, Loss: 0.004994129296392202\n",
            "******************** Test ********************\n",
            "Step: 6160, Loss: 14.04226016998291, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 412/1000, Step: 6180, Loss: 0.03324376791715622\n",
            "******************** Test ********************\n",
            "Step: 6180, Loss: 14.079606056213379, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 413/1000, Step: 6200, Loss: 0.008573133498430252\n",
            "******************** Test ********************\n",
            "Step: 6200, Loss: 14.055061340332031, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 414/1000, Step: 6220, Loss: 0.008057039231061935\n",
            "******************** Test ********************\n",
            "Step: 6220, Loss: 14.065506935119629, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 416/1000, Step: 6240, Loss: 0.008182914927601814\n",
            "******************** Test ********************\n",
            "Step: 6240, Loss: 14.123976707458496, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 417/1000, Step: 6260, Loss: 0.04086001217365265\n",
            "******************** Test ********************\n",
            "Step: 6260, Loss: 14.170522689819336, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 418/1000, Step: 6280, Loss: 0.013955272734165192\n",
            "******************** Test ********************\n",
            "Step: 6280, Loss: 14.194830894470215, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 420/1000, Step: 6300, Loss: 0.006672098767012358\n",
            "******************** Test ********************\n",
            "Step: 6300, Loss: 14.288020133972168, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 421/1000, Step: 6320, Loss: 0.010437176562845707\n",
            "******************** Test ********************\n",
            "Step: 6320, Loss: 14.322211265563965, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 422/1000, Step: 6340, Loss: 0.004997193347662687\n",
            "******************** Test ********************\n",
            "Step: 6340, Loss: 14.316370964050293, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 424/1000, Step: 6360, Loss: 0.004671989940106869\n",
            "******************** Test ********************\n",
            "Step: 6360, Loss: 14.356184959411621, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 425/1000, Step: 6380, Loss: 0.018690213561058044\n",
            "******************** Test ********************\n",
            "Step: 6380, Loss: 14.38428020477295, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 426/1000, Step: 6400, Loss: 0.026800790801644325\n",
            "******************** Test ********************\n",
            "Step: 6400, Loss: 14.475398063659668, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 428/1000, Step: 6420, Loss: 0.021269775927066803\n",
            "******************** Test ********************\n",
            "Step: 6420, Loss: 14.523124694824219, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 429/1000, Step: 6440, Loss: 0.006891387049108744\n",
            "******************** Test ********************\n",
            "Step: 6440, Loss: 14.5389986038208, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 430/1000, Step: 6460, Loss: 0.015506014227867126\n",
            "******************** Test ********************\n",
            "Step: 6460, Loss: 14.533125877380371, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 432/1000, Step: 6480, Loss: 0.012945643626153469\n",
            "******************** Test ********************\n",
            "Step: 6480, Loss: 14.596305847167969, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 433/1000, Step: 6500, Loss: 0.008991721086204052\n",
            "******************** Test ********************\n",
            "Step: 6500, Loss: 14.627333641052246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 434/1000, Step: 6520, Loss: 0.015476560220122337\n",
            "******************** Test ********************\n",
            "Step: 6520, Loss: 14.631620407104492, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 436/1000, Step: 6540, Loss: 0.004652991890907288\n",
            "******************** Test ********************\n",
            "Step: 6540, Loss: 14.653450012207031, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 437/1000, Step: 6560, Loss: 0.01659277267754078\n",
            "******************** Test ********************\n",
            "Step: 6560, Loss: 14.674112319946289, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 438/1000, Step: 6580, Loss: 0.102226123213768\n",
            "******************** Test ********************\n",
            "Step: 6580, Loss: 14.75915813446045, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 440/1000, Step: 6600, Loss: 0.005659680813550949\n",
            "******************** Test ********************\n",
            "Step: 6600, Loss: 14.788235664367676, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 441/1000, Step: 6620, Loss: 0.02035493589937687\n",
            "******************** Test ********************\n",
            "Step: 6620, Loss: 14.821152687072754, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 442/1000, Step: 6640, Loss: 0.0023367542307823896\n",
            "******************** Test ********************\n",
            "Step: 6640, Loss: 14.880779266357422, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 444/1000, Step: 6660, Loss: 0.012738432735204697\n",
            "******************** Test ********************\n",
            "Step: 6660, Loss: 14.919981002807617, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 445/1000, Step: 6680, Loss: 0.008983023464679718\n",
            "******************** Test ********************\n",
            "Step: 6680, Loss: 14.977919578552246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 446/1000, Step: 6700, Loss: 0.0062584239058196545\n",
            "******************** Test ********************\n",
            "Step: 6700, Loss: 14.963966369628906, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 448/1000, Step: 6720, Loss: 0.00478629395365715\n",
            "******************** Test ********************\n",
            "Step: 6720, Loss: 15.036886215209961, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 449/1000, Step: 6740, Loss: 0.007744579575955868\n",
            "******************** Test ********************\n",
            "Step: 6740, Loss: 15.120039939880371, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 450/1000, Step: 6760, Loss: 0.005965841468423605\n",
            "******************** Test ********************\n",
            "Step: 6760, Loss: 15.087343215942383, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 452/1000, Step: 6780, Loss: 0.005431554280221462\n",
            "******************** Test ********************\n",
            "Step: 6780, Loss: 15.086625099182129, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 9s): 453/1000, Step: 6800, Loss: 0.006534392945468426\n",
            "******************** Test ********************\n",
            "Step: 6800, Loss: 15.174711227416992, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 454/1000, Step: 6820, Loss: 0.007208926137536764\n",
            "******************** Test ********************\n",
            "Step: 6820, Loss: 15.156420707702637, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 456/1000, Step: 6840, Loss: 0.010766923427581787\n",
            "******************** Test ********************\n",
            "Step: 6840, Loss: 15.243349075317383, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 457/1000, Step: 6860, Loss: 0.01298576034605503\n",
            "******************** Test ********************\n",
            "Step: 6860, Loss: 15.301603317260742, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 458/1000, Step: 6880, Loss: 0.006268453784286976\n",
            "******************** Test ********************\n",
            "Step: 6880, Loss: 15.291109085083008, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 460/1000, Step: 6900, Loss: 0.004784971475601196\n",
            "******************** Test ********************\n",
            "Step: 6900, Loss: 15.312432289123535, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 461/1000, Step: 6920, Loss: 0.004497005138546228\n",
            "******************** Test ********************\n",
            "Step: 6920, Loss: 15.341054916381836, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 462/1000, Step: 6940, Loss: 0.034631453454494476\n",
            "******************** Test ********************\n",
            "Step: 6940, Loss: 15.449355125427246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 464/1000, Step: 6960, Loss: 0.005327385850250721\n",
            "******************** Test ********************\n",
            "Step: 6960, Loss: 15.421216011047363, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 465/1000, Step: 6980, Loss: 0.013503187336027622\n",
            "******************** Test ********************\n",
            "Step: 6980, Loss: 15.4451904296875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 466/1000, Step: 7000, Loss: 0.04552390053868294\n",
            "******************** Test ********************\n",
            "Step: 7000, Loss: 15.541232109069824, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 468/1000, Step: 7020, Loss: 0.004688706248998642\n",
            "******************** Test ********************\n",
            "Step: 7020, Loss: 15.520404815673828, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 469/1000, Step: 7040, Loss: 0.01831101067364216\n",
            "******************** Test ********************\n",
            "Step: 7040, Loss: 15.581416130065918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 470/1000, Step: 7060, Loss: 0.0026278949808329344\n",
            "******************** Test ********************\n",
            "Step: 7060, Loss: 15.55673885345459, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 472/1000, Step: 7080, Loss: 0.006750420201569796\n",
            "******************** Test ********************\n",
            "Step: 7080, Loss: 15.658215522766113, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 473/1000, Step: 7100, Loss: 0.03388292342424393\n",
            "******************** Test ********************\n",
            "Step: 7100, Loss: 15.713756561279297, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 474/1000, Step: 7120, Loss: 0.008383960463106632\n",
            "******************** Test ********************\n",
            "Step: 7120, Loss: 15.785922050476074, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 476/1000, Step: 7140, Loss: 0.0035002268850803375\n",
            "******************** Test ********************\n",
            "Step: 7140, Loss: 15.760730743408203, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 477/1000, Step: 7160, Loss: 0.04215522110462189\n",
            "******************** Test ********************\n",
            "Step: 7160, Loss: 15.768155097961426, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 478/1000, Step: 7180, Loss: 0.006658155936747789\n",
            "******************** Test ********************\n",
            "Step: 7180, Loss: 15.835777282714844, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 480/1000, Step: 7200, Loss: 0.00759508740156889\n",
            "******************** Test ********************\n",
            "Step: 7200, Loss: 15.873046875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 481/1000, Step: 7220, Loss: 0.002345495857298374\n",
            "******************** Test ********************\n",
            "Step: 7220, Loss: 15.894145965576172, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 482/1000, Step: 7240, Loss: 0.004102836828678846\n",
            "******************** Test ********************\n",
            "Step: 7240, Loss: 15.93243408203125, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 484/1000, Step: 7260, Loss: 0.029474657028913498\n",
            "******************** Test ********************\n",
            "Step: 7260, Loss: 15.902705192565918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 485/1000, Step: 7280, Loss: 0.005126430187374353\n",
            "******************** Test ********************\n",
            "Step: 7280, Loss: 15.979571342468262, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 486/1000, Step: 7300, Loss: 0.012155218049883842\n",
            "******************** Test ********************\n",
            "Step: 7300, Loss: 15.973226547241211, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 488/1000, Step: 7320, Loss: 0.026157883927226067\n",
            "******************** Test ********************\n",
            "Step: 7320, Loss: 16.042808532714844, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 489/1000, Step: 7340, Loss: 0.0029644500464200974\n",
            "******************** Test ********************\n",
            "Step: 7340, Loss: 16.047372817993164, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 490/1000, Step: 7360, Loss: 0.003687504678964615\n",
            "******************** Test ********************\n",
            "Step: 7360, Loss: 16.036813735961914, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 492/1000, Step: 7380, Loss: 0.006369629874825478\n",
            "******************** Test ********************\n",
            "Step: 7380, Loss: 16.157575607299805, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 493/1000, Step: 7400, Loss: 0.005153243895620108\n",
            "******************** Test ********************\n",
            "Step: 7400, Loss: 16.198957443237305, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 494/1000, Step: 7420, Loss: 0.003350325394421816\n",
            "******************** Test ********************\n",
            "Step: 7420, Loss: 16.208778381347656, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 496/1000, Step: 7440, Loss: 0.007227005902677774\n",
            "******************** Test ********************\n",
            "Step: 7440, Loss: 16.247161865234375, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 497/1000, Step: 7460, Loss: 0.0011142886942252517\n",
            "******************** Test ********************\n",
            "Step: 7460, Loss: 16.2531795501709, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 498/1000, Step: 7480, Loss: 0.029294610023498535\n",
            "******************** Test ********************\n",
            "Step: 7480, Loss: 16.311077117919922, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 10s): 500/1000, Step: 7500, Loss: 0.0033593291882425547\n",
            "******************** Test ********************\n",
            "Step: 7500, Loss: 16.3898868560791, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 501/1000, Step: 7520, Loss: 0.0030455095693469048\n",
            "******************** Test ********************\n",
            "Step: 7520, Loss: 16.408540725708008, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 502/1000, Step: 7540, Loss: 0.023811353370547295\n",
            "******************** Test ********************\n",
            "Step: 7540, Loss: 16.412385940551758, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 504/1000, Step: 7560, Loss: 0.006273754872381687\n",
            "******************** Test ********************\n",
            "Step: 7560, Loss: 16.416404724121094, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 505/1000, Step: 7580, Loss: 0.013188132084906101\n",
            "******************** Test ********************\n",
            "Step: 7580, Loss: 16.456201553344727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 506/1000, Step: 7600, Loss: 0.0042771631851792336\n",
            "******************** Test ********************\n",
            "Step: 7600, Loss: 16.473806381225586, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 508/1000, Step: 7620, Loss: 0.012207606807351112\n",
            "******************** Test ********************\n",
            "Step: 7620, Loss: 16.53163719177246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 509/1000, Step: 7640, Loss: 0.014593326486647129\n",
            "******************** Test ********************\n",
            "Step: 7640, Loss: 16.560287475585938, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 510/1000, Step: 7660, Loss: 0.0034781889989972115\n",
            "******************** Test ********************\n",
            "Step: 7660, Loss: 16.654691696166992, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 512/1000, Step: 7680, Loss: 0.002586337272077799\n",
            "******************** Test ********************\n",
            "Step: 7680, Loss: 16.60173988342285, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 513/1000, Step: 7700, Loss: 0.005118489731103182\n",
            "******************** Test ********************\n",
            "Step: 7700, Loss: 16.55192756652832, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 514/1000, Step: 7720, Loss: 0.016451116651296616\n",
            "******************** Test ********************\n",
            "Step: 7720, Loss: 16.63015365600586, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 516/1000, Step: 7740, Loss: 0.0044356235302984715\n",
            "******************** Test ********************\n",
            "Step: 7740, Loss: 16.714689254760742, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 517/1000, Step: 7760, Loss: 0.0019847110379487276\n",
            "******************** Test ********************\n",
            "Step: 7760, Loss: 16.77100372314453, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 518/1000, Step: 7780, Loss: 0.0030942901503294706\n",
            "******************** Test ********************\n",
            "Step: 7780, Loss: 16.806732177734375, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 520/1000, Step: 7800, Loss: 0.023466387763619423\n",
            "******************** Test ********************\n",
            "Step: 7800, Loss: 16.809932708740234, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 521/1000, Step: 7820, Loss: 0.025913085788488388\n",
            "******************** Test ********************\n",
            "Step: 7820, Loss: 16.85084342956543, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 522/1000, Step: 7840, Loss: 0.008120016194880009\n",
            "******************** Test ********************\n",
            "Step: 7840, Loss: 16.880203247070312, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 524/1000, Step: 7860, Loss: 0.015964357182383537\n",
            "******************** Test ********************\n",
            "Step: 7860, Loss: 16.88823890686035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 525/1000, Step: 7880, Loss: 0.005432602018117905\n",
            "******************** Test ********************\n",
            "Step: 7880, Loss: 16.916759490966797, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 526/1000, Step: 7900, Loss: 0.002100864192470908\n",
            "******************** Test ********************\n",
            "Step: 7900, Loss: 16.98166847229004, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 528/1000, Step: 7920, Loss: 0.014472778886556625\n",
            "******************** Test ********************\n",
            "Step: 7920, Loss: 17.004743576049805, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 529/1000, Step: 7940, Loss: 0.002043907530605793\n",
            "******************** Test ********************\n",
            "Step: 7940, Loss: 17.019426345825195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 530/1000, Step: 7960, Loss: 0.0015497892163693905\n",
            "******************** Test ********************\n",
            "Step: 7960, Loss: 17.06602668762207, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 532/1000, Step: 7980, Loss: 0.0016609040321782231\n",
            "******************** Test ********************\n",
            "Step: 7980, Loss: 17.105457305908203, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 533/1000, Step: 8000, Loss: 0.015846064314246178\n",
            "******************** Test ********************\n",
            "Step: 8000, Loss: 17.080936431884766, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 534/1000, Step: 8020, Loss: 0.002827145392075181\n",
            "******************** Test ********************\n",
            "Step: 8020, Loss: 17.155601501464844, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 536/1000, Step: 8040, Loss: 0.0077285426668822765\n",
            "******************** Test ********************\n",
            "Step: 8040, Loss: 17.15958023071289, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 537/1000, Step: 8060, Loss: 0.0016114829340949655\n",
            "******************** Test ********************\n",
            "Step: 8060, Loss: 17.18321990966797, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 538/1000, Step: 8080, Loss: 0.0025711078196763992\n",
            "******************** Test ********************\n",
            "Step: 8080, Loss: 17.227588653564453, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 540/1000, Step: 8100, Loss: 0.004682661034166813\n",
            "******************** Test ********************\n",
            "Step: 8100, Loss: 17.294200897216797, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 541/1000, Step: 8120, Loss: 0.002989061875268817\n",
            "******************** Test ********************\n",
            "Step: 8120, Loss: 17.29505157470703, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 542/1000, Step: 8140, Loss: 0.004961298778653145\n",
            "******************** Test ********************\n",
            "Step: 8140, Loss: 17.288881301879883, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 544/1000, Step: 8160, Loss: 0.002634264761582017\n",
            "******************** Test ********************\n",
            "Step: 8160, Loss: 17.34833335876465, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 545/1000, Step: 8180, Loss: 0.003504210151731968\n",
            "******************** Test ********************\n",
            "Step: 8180, Loss: 17.408077239990234, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 11s): 546/1000, Step: 8200, Loss: 0.0027493671514093876\n",
            "******************** Test ********************\n",
            "Step: 8200, Loss: 17.44367218017578, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 548/1000, Step: 8220, Loss: 0.007951043546199799\n",
            "******************** Test ********************\n",
            "Step: 8220, Loss: 17.467044830322266, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 549/1000, Step: 8240, Loss: 0.003739988664165139\n",
            "******************** Test ********************\n",
            "Step: 8240, Loss: 17.501386642456055, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 550/1000, Step: 8260, Loss: 0.0035461988300085068\n",
            "******************** Test ********************\n",
            "Step: 8260, Loss: 17.53533363342285, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 552/1000, Step: 8280, Loss: 0.0028762705624103546\n",
            "******************** Test ********************\n",
            "Step: 8280, Loss: 17.583572387695312, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 553/1000, Step: 8300, Loss: 0.0028187891002744436\n",
            "******************** Test ********************\n",
            "Step: 8300, Loss: 17.645219802856445, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 554/1000, Step: 8320, Loss: 0.0028600285295397043\n",
            "******************** Test ********************\n",
            "Step: 8320, Loss: 17.636789321899414, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 556/1000, Step: 8340, Loss: 0.002088325098156929\n",
            "******************** Test ********************\n",
            "Step: 8340, Loss: 17.65097999572754, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 557/1000, Step: 8360, Loss: 0.001628920785151422\n",
            "******************** Test ********************\n",
            "Step: 8360, Loss: 17.68375015258789, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 558/1000, Step: 8380, Loss: 0.002204088494181633\n",
            "******************** Test ********************\n",
            "Step: 8380, Loss: 17.709104537963867, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 560/1000, Step: 8400, Loss: 0.012623087503015995\n",
            "******************** Test ********************\n",
            "Step: 8400, Loss: 17.719144821166992, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 561/1000, Step: 8420, Loss: 0.003276231698691845\n",
            "******************** Test ********************\n",
            "Step: 8420, Loss: 17.74854850769043, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 562/1000, Step: 8440, Loss: 0.005816427990794182\n",
            "******************** Test ********************\n",
            "Step: 8440, Loss: 17.748981475830078, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 564/1000, Step: 8460, Loss: 0.002228722907602787\n",
            "******************** Test ********************\n",
            "Step: 8460, Loss: 17.810501098632812, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 565/1000, Step: 8480, Loss: 0.004350572824478149\n",
            "******************** Test ********************\n",
            "Step: 8480, Loss: 17.853818893432617, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 566/1000, Step: 8500, Loss: 0.006584633141756058\n",
            "******************** Test ********************\n",
            "Step: 8500, Loss: 17.886470794677734, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 568/1000, Step: 8520, Loss: 0.010240561328828335\n",
            "******************** Test ********************\n",
            "Step: 8520, Loss: 17.939062118530273, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 569/1000, Step: 8540, Loss: 0.02036132477223873\n",
            "******************** Test ********************\n",
            "Step: 8540, Loss: 17.89789581298828, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 570/1000, Step: 8560, Loss: 0.009537328034639359\n",
            "******************** Test ********************\n",
            "Step: 8560, Loss: 17.932409286499023, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 572/1000, Step: 8580, Loss: 0.0015903187450021505\n",
            "******************** Test ********************\n",
            "Step: 8580, Loss: 18.016637802124023, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 573/1000, Step: 8600, Loss: 0.005333374720066786\n",
            "******************** Test ********************\n",
            "Step: 8600, Loss: 18.05571937561035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 574/1000, Step: 8620, Loss: 0.009882550686597824\n",
            "******************** Test ********************\n",
            "Step: 8620, Loss: 18.084436416625977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 576/1000, Step: 8640, Loss: 0.0011751444544643164\n",
            "******************** Test ********************\n",
            "Step: 8640, Loss: 18.11823081970215, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 577/1000, Step: 8660, Loss: 0.007087203208357096\n",
            "******************** Test ********************\n",
            "Step: 8660, Loss: 18.112520217895508, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 578/1000, Step: 8680, Loss: 0.0014375081518664956\n",
            "******************** Test ********************\n",
            "Step: 8680, Loss: 18.15257453918457, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 580/1000, Step: 8700, Loss: 0.0013111367588862777\n",
            "******************** Test ********************\n",
            "Step: 8700, Loss: 18.187484741210938, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 581/1000, Step: 8720, Loss: 0.002177674090489745\n",
            "******************** Test ********************\n",
            "Step: 8720, Loss: 18.23729705810547, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 582/1000, Step: 8740, Loss: 0.0022218504454940557\n",
            "******************** Test ********************\n",
            "Step: 8740, Loss: 18.31296730041504, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 584/1000, Step: 8760, Loss: 0.0016695833764970303\n",
            "******************** Test ********************\n",
            "Step: 8760, Loss: 18.276363372802734, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 585/1000, Step: 8780, Loss: 0.007128917146474123\n",
            "******************** Test ********************\n",
            "Step: 8780, Loss: 18.32651710510254, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 586/1000, Step: 8800, Loss: 0.0008796411566436291\n",
            "******************** Test ********************\n",
            "Step: 8800, Loss: 18.38639259338379, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 588/1000, Step: 8820, Loss: 0.0011670374078676105\n",
            "******************** Test ********************\n",
            "Step: 8820, Loss: 18.35342788696289, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 589/1000, Step: 8840, Loss: 0.00041692479862831533\n",
            "******************** Test ********************\n",
            "Step: 8840, Loss: 18.39434051513672, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 590/1000, Step: 8860, Loss: 0.006095591001212597\n",
            "******************** Test ********************\n",
            "Step: 8860, Loss: 18.41450309753418, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 592/1000, Step: 8880, Loss: 0.001666999771259725\n",
            "******************** Test ********************\n",
            "Step: 8880, Loss: 18.424551010131836, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 12s): 593/1000, Step: 8900, Loss: 0.002103538252413273\n",
            "******************** Test ********************\n",
            "Step: 8900, Loss: 18.46413803100586, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 594/1000, Step: 8920, Loss: 0.0011337227188050747\n",
            "******************** Test ********************\n",
            "Step: 8920, Loss: 18.545822143554688, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 596/1000, Step: 8940, Loss: 0.0015806654701009393\n",
            "******************** Test ********************\n",
            "Step: 8940, Loss: 18.55767822265625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 597/1000, Step: 8960, Loss: 0.0007681453716941178\n",
            "******************** Test ********************\n",
            "Step: 8960, Loss: 18.580753326416016, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 598/1000, Step: 8980, Loss: 0.0019071485148742795\n",
            "******************** Test ********************\n",
            "Step: 8980, Loss: 18.66632843017578, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 600/1000, Step: 9000, Loss: 0.003587279701605439\n",
            "******************** Test ********************\n",
            "Step: 9000, Loss: 18.659645080566406, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 601/1000, Step: 9020, Loss: 0.001620155293494463\n",
            "******************** Test ********************\n",
            "Step: 9020, Loss: 18.69304847717285, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 602/1000, Step: 9040, Loss: 0.0032482300885021687\n",
            "******************** Test ********************\n",
            "Step: 9040, Loss: 18.658756256103516, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 604/1000, Step: 9060, Loss: 0.008313842117786407\n",
            "******************** Test ********************\n",
            "Step: 9060, Loss: 18.73604393005371, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 605/1000, Step: 9080, Loss: 0.005939459428191185\n",
            "******************** Test ********************\n",
            "Step: 9080, Loss: 18.800479888916016, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 606/1000, Step: 9100, Loss: 0.0018791011534631252\n",
            "******************** Test ********************\n",
            "Step: 9100, Loss: 18.78750228881836, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 608/1000, Step: 9120, Loss: 0.0014036293141543865\n",
            "******************** Test ********************\n",
            "Step: 9120, Loss: 18.8519229888916, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 609/1000, Step: 9140, Loss: 0.0009306696010753512\n",
            "******************** Test ********************\n",
            "Step: 9140, Loss: 18.863876342773438, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 610/1000, Step: 9160, Loss: 0.006693834904581308\n",
            "******************** Test ********************\n",
            "Step: 9160, Loss: 18.879735946655273, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 612/1000, Step: 9180, Loss: 0.0012271777959540486\n",
            "******************** Test ********************\n",
            "Step: 9180, Loss: 18.902896881103516, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 613/1000, Step: 9200, Loss: 0.00934663787484169\n",
            "******************** Test ********************\n",
            "Step: 9200, Loss: 18.927337646484375, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 614/1000, Step: 9220, Loss: 0.0009174340520985425\n",
            "******************** Test ********************\n",
            "Step: 9220, Loss: 18.9914493560791, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 616/1000, Step: 9240, Loss: 0.0009393217624165118\n",
            "******************** Test ********************\n",
            "Step: 9240, Loss: 18.966672897338867, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 617/1000, Step: 9260, Loss: 0.0012876588152721524\n",
            "******************** Test ********************\n",
            "Step: 9260, Loss: 19.055315017700195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 618/1000, Step: 9280, Loss: 0.0010518946219235659\n",
            "******************** Test ********************\n",
            "Step: 9280, Loss: 19.08245086669922, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 620/1000, Step: 9300, Loss: 0.0007157873478718102\n",
            "******************** Test ********************\n",
            "Step: 9300, Loss: 19.101343154907227, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 621/1000, Step: 9320, Loss: 0.0032570038456469774\n",
            "******************** Test ********************\n",
            "Step: 9320, Loss: 19.095643997192383, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 622/1000, Step: 9340, Loss: 0.005635065492242575\n",
            "******************** Test ********************\n",
            "Step: 9340, Loss: 19.097335815429688, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 624/1000, Step: 9360, Loss: 0.001964643830433488\n",
            "******************** Test ********************\n",
            "Step: 9360, Loss: 19.18311882019043, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 625/1000, Step: 9380, Loss: 0.0027691891882568598\n",
            "******************** Test ********************\n",
            "Step: 9380, Loss: 19.23424530029297, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 626/1000, Step: 9400, Loss: 0.0028474698774516582\n",
            "******************** Test ********************\n",
            "Step: 9400, Loss: 19.265384674072266, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 628/1000, Step: 9420, Loss: 0.0010153382318094373\n",
            "******************** Test ********************\n",
            "Step: 9420, Loss: 19.270231246948242, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 629/1000, Step: 9440, Loss: 0.002832456724718213\n",
            "******************** Test ********************\n",
            "Step: 9440, Loss: 19.28386878967285, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 630/1000, Step: 9460, Loss: 0.0006994637660682201\n",
            "******************** Test ********************\n",
            "Step: 9460, Loss: 19.3018798828125, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 632/1000, Step: 9480, Loss: 0.0015407047467306256\n",
            "******************** Test ********************\n",
            "Step: 9480, Loss: 19.387453079223633, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 633/1000, Step: 9500, Loss: 0.0008894460625015199\n",
            "******************** Test ********************\n",
            "Step: 9500, Loss: 19.378992080688477, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 634/1000, Step: 9520, Loss: 0.0007558178040198982\n",
            "******************** Test ********************\n",
            "Step: 9520, Loss: 19.43377685546875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 636/1000, Step: 9540, Loss: 0.0011145989410579205\n",
            "******************** Test ********************\n",
            "Step: 9540, Loss: 19.451183319091797, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 637/1000, Step: 9560, Loss: 0.0049181655049324036\n",
            "******************** Test ********************\n",
            "Step: 9560, Loss: 19.459028244018555, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 638/1000, Step: 9580, Loss: 0.0008220099261961877\n",
            "******************** Test ********************\n",
            "Step: 9580, Loss: 19.50786781311035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 640/1000, Step: 9600, Loss: 0.0008072504424490035\n",
            "******************** Test ********************\n",
            "Step: 9600, Loss: 19.52661895751953, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 13s): 641/1000, Step: 9620, Loss: 0.004925623070448637\n",
            "******************** Test ********************\n",
            "Step: 9620, Loss: 19.507165908813477, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 642/1000, Step: 9640, Loss: 0.000991140492260456\n",
            "******************** Test ********************\n",
            "Step: 9640, Loss: 19.564905166625977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 644/1000, Step: 9660, Loss: 0.0029027273412793875\n",
            "******************** Test ********************\n",
            "Step: 9660, Loss: 19.604541778564453, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 645/1000, Step: 9680, Loss: 0.0023058264050632715\n",
            "******************** Test ********************\n",
            "Step: 9680, Loss: 19.62751007080078, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 646/1000, Step: 9700, Loss: 0.005182088352739811\n",
            "******************** Test ********************\n",
            "Step: 9700, Loss: 19.644010543823242, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 648/1000, Step: 9720, Loss: 0.0033720110077410936\n",
            "******************** Test ********************\n",
            "Step: 9720, Loss: 19.738388061523438, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 649/1000, Step: 9740, Loss: 0.005260934587568045\n",
            "******************** Test ********************\n",
            "Step: 9740, Loss: 19.750028610229492, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 650/1000, Step: 9760, Loss: 0.003046143101528287\n",
            "******************** Test ********************\n",
            "Step: 9760, Loss: 19.830936431884766, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 652/1000, Step: 9780, Loss: 0.001768043846823275\n",
            "******************** Test ********************\n",
            "Step: 9780, Loss: 19.78999900817871, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 653/1000, Step: 9800, Loss: 0.0007513326709158719\n",
            "******************** Test ********************\n",
            "Step: 9800, Loss: 19.851783752441406, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 654/1000, Step: 9820, Loss: 0.0020028529688715935\n",
            "******************** Test ********************\n",
            "Step: 9820, Loss: 19.85984230041504, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 656/1000, Step: 9840, Loss: 0.003413402708247304\n",
            "******************** Test ********************\n",
            "Step: 9840, Loss: 19.883947372436523, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 657/1000, Step: 9860, Loss: 0.0021922343876212835\n",
            "******************** Test ********************\n",
            "Step: 9860, Loss: 19.938718795776367, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 658/1000, Step: 9880, Loss: 0.001243709004484117\n",
            "******************** Test ********************\n",
            "Step: 9880, Loss: 19.925613403320312, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 660/1000, Step: 9900, Loss: 0.004267585929483175\n",
            "******************** Test ********************\n",
            "Step: 9900, Loss: 19.9678897857666, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 661/1000, Step: 9920, Loss: 0.0008693650015629828\n",
            "******************** Test ********************\n",
            "Step: 9920, Loss: 20.021818161010742, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 662/1000, Step: 9940, Loss: 0.0010037319734692574\n",
            "******************** Test ********************\n",
            "Step: 9940, Loss: 20.084163665771484, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 664/1000, Step: 9960, Loss: 0.0004236773238517344\n",
            "******************** Test ********************\n",
            "Step: 9960, Loss: 20.101943969726562, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 665/1000, Step: 9980, Loss: 0.002877316437661648\n",
            "******************** Test ********************\n",
            "Step: 9980, Loss: 20.087814331054688, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 666/1000, Step: 10000, Loss: 0.003466143738478422\n",
            "******************** Test ********************\n",
            "Step: 10000, Loss: 20.077728271484375, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 668/1000, Step: 10020, Loss: 0.003908618353307247\n",
            "******************** Test ********************\n",
            "Step: 10020, Loss: 20.148046493530273, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 669/1000, Step: 10040, Loss: 0.0006230510771274567\n",
            "******************** Test ********************\n",
            "Step: 10040, Loss: 20.13561248779297, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 670/1000, Step: 10060, Loss: 0.0008357453043572605\n",
            "******************** Test ********************\n",
            "Step: 10060, Loss: 20.188570022583008, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 672/1000, Step: 10080, Loss: 0.0005487824673764408\n",
            "******************** Test ********************\n",
            "Step: 10080, Loss: 20.2438907623291, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 673/1000, Step: 10100, Loss: 0.0003053357359021902\n",
            "******************** Test ********************\n",
            "Step: 10100, Loss: 20.222206115722656, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 674/1000, Step: 10120, Loss: 0.004830343648791313\n",
            "******************** Test ********************\n",
            "Step: 10120, Loss: 20.238452911376953, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 676/1000, Step: 10140, Loss: 0.0005253502749837935\n",
            "******************** Test ********************\n",
            "Step: 10140, Loss: 20.299291610717773, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 677/1000, Step: 10160, Loss: 0.0003214833268430084\n",
            "******************** Test ********************\n",
            "Step: 10160, Loss: 20.35958480834961, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 678/1000, Step: 10180, Loss: 0.0005459071253426373\n",
            "******************** Test ********************\n",
            "Step: 10180, Loss: 20.38176727294922, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 680/1000, Step: 10200, Loss: 0.0005270185647532344\n",
            "******************** Test ********************\n",
            "Step: 10200, Loss: 20.41950035095215, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 681/1000, Step: 10220, Loss: 0.0007663923315703869\n",
            "******************** Test ********************\n",
            "Step: 10220, Loss: 20.4161319732666, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 682/1000, Step: 10240, Loss: 0.0003449209616519511\n",
            "******************** Test ********************\n",
            "Step: 10240, Loss: 20.479450225830078, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 684/1000, Step: 10260, Loss: 0.0004425518272910267\n",
            "******************** Test ********************\n",
            "Step: 10260, Loss: 20.493797302246094, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 14s): 685/1000, Step: 10280, Loss: 0.00091066665481776\n",
            "******************** Test ********************\n",
            "Step: 10280, Loss: 20.551130294799805, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 686/1000, Step: 10300, Loss: 0.00047179951798170805\n",
            "******************** Test ********************\n",
            "Step: 10300, Loss: 20.520992279052734, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 688/1000, Step: 10320, Loss: 0.0023804837837815285\n",
            "******************** Test ********************\n",
            "Step: 10320, Loss: 20.529205322265625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 689/1000, Step: 10340, Loss: 0.0008617485291324556\n",
            "******************** Test ********************\n",
            "Step: 10340, Loss: 20.570022583007812, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 690/1000, Step: 10360, Loss: 0.0009129403624683619\n",
            "******************** Test ********************\n",
            "Step: 10360, Loss: 20.603862762451172, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 692/1000, Step: 10380, Loss: 0.0004257463151589036\n",
            "******************** Test ********************\n",
            "Step: 10380, Loss: 20.705001831054688, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 693/1000, Step: 10400, Loss: 0.0010693666990846395\n",
            "******************** Test ********************\n",
            "Step: 10400, Loss: 20.645587921142578, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 694/1000, Step: 10420, Loss: 0.0027209266554564238\n",
            "******************** Test ********************\n",
            "Step: 10420, Loss: 20.69584846496582, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 696/1000, Step: 10440, Loss: 0.0014239316806197166\n",
            "******************** Test ********************\n",
            "Step: 10440, Loss: 20.769697189331055, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 697/1000, Step: 10460, Loss: 0.0005917062517255545\n",
            "******************** Test ********************\n",
            "Step: 10460, Loss: 20.797565460205078, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 698/1000, Step: 10480, Loss: 0.0009095442364923656\n",
            "******************** Test ********************\n",
            "Step: 10480, Loss: 20.79891586303711, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 700/1000, Step: 10500, Loss: 0.0009591394918970764\n",
            "******************** Test ********************\n",
            "Step: 10500, Loss: 20.872705459594727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 701/1000, Step: 10520, Loss: 0.0009798143291845918\n",
            "******************** Test ********************\n",
            "Step: 10520, Loss: 20.86941146850586, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 702/1000, Step: 10540, Loss: 0.0008428437868133187\n",
            "******************** Test ********************\n",
            "Step: 10540, Loss: 20.88654136657715, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 704/1000, Step: 10560, Loss: 0.002108300570398569\n",
            "******************** Test ********************\n",
            "Step: 10560, Loss: 20.900711059570312, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 705/1000, Step: 10580, Loss: 0.004390123300254345\n",
            "******************** Test ********************\n",
            "Step: 10580, Loss: 20.941556930541992, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 706/1000, Step: 10600, Loss: 0.001053480664268136\n",
            "******************** Test ********************\n",
            "Step: 10600, Loss: 20.9675350189209, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 708/1000, Step: 10620, Loss: 0.0018892056541517377\n",
            "******************** Test ********************\n",
            "Step: 10620, Loss: 20.95891761779785, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 709/1000, Step: 10640, Loss: 0.0005229130620136857\n",
            "******************** Test ********************\n",
            "Step: 10640, Loss: 20.988121032714844, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 710/1000, Step: 10660, Loss: 0.003205950139090419\n",
            "******************** Test ********************\n",
            "Step: 10660, Loss: 21.011398315429688, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 712/1000, Step: 10680, Loss: 0.0007945189718157053\n",
            "******************** Test ********************\n",
            "Step: 10680, Loss: 21.061769485473633, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 713/1000, Step: 10700, Loss: 0.0012497580610215664\n",
            "******************** Test ********************\n",
            "Step: 10700, Loss: 21.056976318359375, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 714/1000, Step: 10720, Loss: 0.0006390614435076714\n",
            "******************** Test ********************\n",
            "Step: 10720, Loss: 21.156570434570312, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 716/1000, Step: 10740, Loss: 0.001554879010654986\n",
            "******************** Test ********************\n",
            "Step: 10740, Loss: 21.140060424804688, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 717/1000, Step: 10760, Loss: 0.00020736173610202968\n",
            "******************** Test ********************\n",
            "Step: 10760, Loss: 21.165361404418945, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 718/1000, Step: 10780, Loss: 0.0025444733910262585\n",
            "******************** Test ********************\n",
            "Step: 10780, Loss: 21.202478408813477, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 720/1000, Step: 10800, Loss: 0.0002669149835128337\n",
            "******************** Test ********************\n",
            "Step: 10800, Loss: 21.235132217407227, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 721/1000, Step: 10820, Loss: 0.0009383464930579066\n",
            "******************** Test ********************\n",
            "Step: 10820, Loss: 21.259849548339844, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 722/1000, Step: 10840, Loss: 0.0007334728143177927\n",
            "******************** Test ********************\n",
            "Step: 10840, Loss: 21.321622848510742, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 724/1000, Step: 10860, Loss: 0.0005020964308641851\n",
            "******************** Test ********************\n",
            "Step: 10860, Loss: 21.342187881469727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 725/1000, Step: 10880, Loss: 0.0003021032898686826\n",
            "******************** Test ********************\n",
            "Step: 10880, Loss: 21.36760139465332, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 15s): 726/1000, Step: 10900, Loss: 0.0005942325224168599\n",
            "******************** Test ********************\n",
            "Step: 10900, Loss: 21.369997024536133, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 728/1000, Step: 10920, Loss: 0.0005450081662274897\n",
            "******************** Test ********************\n",
            "Step: 10920, Loss: 21.4068660736084, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 729/1000, Step: 10940, Loss: 0.0021444279700517654\n",
            "******************** Test ********************\n",
            "Step: 10940, Loss: 21.42231559753418, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 730/1000, Step: 10960, Loss: 0.000746318488381803\n",
            "******************** Test ********************\n",
            "Step: 10960, Loss: 21.46543312072754, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 732/1000, Step: 10980, Loss: 0.0005623464821837842\n",
            "******************** Test ********************\n",
            "Step: 10980, Loss: 21.46699333190918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 733/1000, Step: 11000, Loss: 0.0003925352357327938\n",
            "******************** Test ********************\n",
            "Step: 11000, Loss: 21.535320281982422, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 734/1000, Step: 11020, Loss: 0.0009876032127067447\n",
            "******************** Test ********************\n",
            "Step: 11020, Loss: 21.510066986083984, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 736/1000, Step: 11040, Loss: 0.00023877181229181588\n",
            "******************** Test ********************\n",
            "Step: 11040, Loss: 21.545299530029297, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 737/1000, Step: 11060, Loss: 0.00026068539591506124\n",
            "******************** Test ********************\n",
            "Step: 11060, Loss: 21.533281326293945, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 738/1000, Step: 11080, Loss: 0.0007221858249977231\n",
            "******************** Test ********************\n",
            "Step: 11080, Loss: 21.595972061157227, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 740/1000, Step: 11100, Loss: 0.00036327645648270845\n",
            "******************** Test ********************\n",
            "Step: 11100, Loss: 21.64958381652832, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 741/1000, Step: 11120, Loss: 0.0006012280937284231\n",
            "******************** Test ********************\n",
            "Step: 11120, Loss: 21.712736129760742, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 742/1000, Step: 11140, Loss: 0.0005412729224190116\n",
            "******************** Test ********************\n",
            "Step: 11140, Loss: 21.696802139282227, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 744/1000, Step: 11160, Loss: 0.0016982951201498508\n",
            "******************** Test ********************\n",
            "Step: 11160, Loss: 21.748735427856445, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 745/1000, Step: 11180, Loss: 0.00029050960438326\n",
            "******************** Test ********************\n",
            "Step: 11180, Loss: 21.780900955200195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 746/1000, Step: 11200, Loss: 0.0003260934026911855\n",
            "******************** Test ********************\n",
            "Step: 11200, Loss: 21.84663963317871, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 748/1000, Step: 11220, Loss: 0.0003159209154546261\n",
            "******************** Test ********************\n",
            "Step: 11220, Loss: 21.84983253479004, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 749/1000, Step: 11240, Loss: 0.0003763573768083006\n",
            "******************** Test ********************\n",
            "Step: 11240, Loss: 21.844457626342773, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 750/1000, Step: 11260, Loss: 0.0012541136238723993\n",
            "******************** Test ********************\n",
            "Step: 11260, Loss: 21.9311466217041, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 752/1000, Step: 11280, Loss: 0.00017876530182547867\n",
            "******************** Test ********************\n",
            "Step: 11280, Loss: 21.924983978271484, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 753/1000, Step: 11300, Loss: 0.00027378732920624316\n",
            "******************** Test ********************\n",
            "Step: 11300, Loss: 21.99964714050293, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 754/1000, Step: 11320, Loss: 0.0003288864973001182\n",
            "******************** Test ********************\n",
            "Step: 11320, Loss: 21.98526954650879, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 756/1000, Step: 11340, Loss: 0.0004884834634140134\n",
            "******************** Test ********************\n",
            "Step: 11340, Loss: 22.04425621032715, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 757/1000, Step: 11360, Loss: 0.00030801663524471223\n",
            "******************** Test ********************\n",
            "Step: 11360, Loss: 22.04311752319336, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 758/1000, Step: 11380, Loss: 0.0007753292447887361\n",
            "******************** Test ********************\n",
            "Step: 11380, Loss: 22.06527328491211, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 760/1000, Step: 11400, Loss: 0.001980018801987171\n",
            "******************** Test ********************\n",
            "Step: 11400, Loss: 22.085927963256836, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 761/1000, Step: 11420, Loss: 0.00043234339682385325\n",
            "******************** Test ********************\n",
            "Step: 11420, Loss: 22.110225677490234, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 762/1000, Step: 11440, Loss: 0.0013925897656008601\n",
            "******************** Test ********************\n",
            "Step: 11440, Loss: 22.190031051635742, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 764/1000, Step: 11460, Loss: 0.001312532345764339\n",
            "******************** Test ********************\n",
            "Step: 11460, Loss: 22.207578659057617, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 765/1000, Step: 11480, Loss: 0.0010239705443382263\n",
            "******************** Test ********************\n",
            "Step: 11480, Loss: 22.202730178833008, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 766/1000, Step: 11500, Loss: 0.0005995607352815568\n",
            "******************** Test ********************\n",
            "Step: 11500, Loss: 22.237594604492188, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 16s): 768/1000, Step: 11520, Loss: 0.00012380957196000963\n",
            "******************** Test ********************\n",
            "Step: 11520, Loss: 22.28215980529785, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 769/1000, Step: 11540, Loss: 0.000592947646509856\n",
            "******************** Test ********************\n",
            "Step: 11540, Loss: 22.34059715270996, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 770/1000, Step: 11560, Loss: 0.0003087378281634301\n",
            "******************** Test ********************\n",
            "Step: 11560, Loss: 22.3350772857666, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 772/1000, Step: 11580, Loss: 0.00029962279950268567\n",
            "******************** Test ********************\n",
            "Step: 11580, Loss: 22.334882736206055, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 773/1000, Step: 11600, Loss: 0.0004915993777103722\n",
            "******************** Test ********************\n",
            "Step: 11600, Loss: 22.316911697387695, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 774/1000, Step: 11620, Loss: 0.0011106288293376565\n",
            "******************** Test ********************\n",
            "Step: 11620, Loss: 22.375877380371094, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 776/1000, Step: 11640, Loss: 0.0004626545123755932\n",
            "******************** Test ********************\n",
            "Step: 11640, Loss: 22.393680572509766, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 777/1000, Step: 11660, Loss: 0.00027675204910337925\n",
            "******************** Test ********************\n",
            "Step: 11660, Loss: 22.4294490814209, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 778/1000, Step: 11680, Loss: 0.0013338800054043531\n",
            "******************** Test ********************\n",
            "Step: 11680, Loss: 22.450302124023438, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 780/1000, Step: 11700, Loss: 0.0011889139423146844\n",
            "******************** Test ********************\n",
            "Step: 11700, Loss: 22.49741554260254, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 781/1000, Step: 11720, Loss: 0.001424130517989397\n",
            "******************** Test ********************\n",
            "Step: 11720, Loss: 22.501691818237305, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 782/1000, Step: 11740, Loss: 0.0006863437592983246\n",
            "******************** Test ********************\n",
            "Step: 11740, Loss: 22.554590225219727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 784/1000, Step: 11760, Loss: 0.00010853025014512241\n",
            "******************** Test ********************\n",
            "Step: 11760, Loss: 22.545602798461914, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 785/1000, Step: 11780, Loss: 0.00032676898990757763\n",
            "******************** Test ********************\n",
            "Step: 11780, Loss: 22.581382751464844, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 786/1000, Step: 11800, Loss: 0.0016754870302975178\n",
            "******************** Test ********************\n",
            "Step: 11800, Loss: 22.641016006469727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 788/1000, Step: 11820, Loss: 0.00028201588429510593\n",
            "******************** Test ********************\n",
            "Step: 11820, Loss: 22.646530151367188, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 789/1000, Step: 11840, Loss: 0.00023672915995121002\n",
            "******************** Test ********************\n",
            "Step: 11840, Loss: 22.708404541015625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 790/1000, Step: 11860, Loss: 0.0011948595056310296\n",
            "******************** Test ********************\n",
            "Step: 11860, Loss: 22.70961570739746, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 792/1000, Step: 11880, Loss: 0.00015439209528267384\n",
            "******************** Test ********************\n",
            "Step: 11880, Loss: 22.75106430053711, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 793/1000, Step: 11900, Loss: 0.0010230598272755742\n",
            "******************** Test ********************\n",
            "Step: 11900, Loss: 22.74874496459961, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 794/1000, Step: 11920, Loss: 0.00023031799355521798\n",
            "******************** Test ********************\n",
            "Step: 11920, Loss: 22.8056583404541, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 796/1000, Step: 11940, Loss: 0.00019425814389251173\n",
            "******************** Test ********************\n",
            "Step: 11940, Loss: 22.83449935913086, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 797/1000, Step: 11960, Loss: 0.000295384437777102\n",
            "******************** Test ********************\n",
            "Step: 11960, Loss: 22.8789119720459, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 798/1000, Step: 11980, Loss: 0.00020269255037419498\n",
            "******************** Test ********************\n",
            "Step: 11980, Loss: 22.92450523376465, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 800/1000, Step: 12000, Loss: 0.00011560501297935843\n",
            "******************** Test ********************\n",
            "Step: 12000, Loss: 22.944255828857422, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 801/1000, Step: 12020, Loss: 0.0002117309340974316\n",
            "******************** Test ********************\n",
            "Step: 12020, Loss: 22.936433792114258, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 802/1000, Step: 12040, Loss: 0.0009938429575413465\n",
            "******************** Test ********************\n",
            "Step: 12040, Loss: 22.930770874023438, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 804/1000, Step: 12060, Loss: 0.0003063990152440965\n",
            "******************** Test ********************\n",
            "Step: 12060, Loss: 22.95497703552246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 805/1000, Step: 12080, Loss: 0.00025040292530320585\n",
            "******************** Test ********************\n",
            "Step: 12080, Loss: 23.018762588500977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 806/1000, Step: 12100, Loss: 0.00022948037076275796\n",
            "******************** Test ********************\n",
            "Step: 12100, Loss: 23.044057846069336, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 808/1000, Step: 12120, Loss: 0.0002852698671631515\n",
            "******************** Test ********************\n",
            "Step: 12120, Loss: 23.07734489440918, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 809/1000, Step: 12140, Loss: 9.102349577005953e-05\n",
            "******************** Test ********************\n",
            "Step: 12140, Loss: 23.103410720825195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 17s): 810/1000, Step: 12160, Loss: 0.00045828803558833897\n",
            "******************** Test ********************\n",
            "Step: 12160, Loss: 23.09345245361328, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 812/1000, Step: 12180, Loss: 0.0008752985158935189\n",
            "******************** Test ********************\n",
            "Step: 12180, Loss: 23.147615432739258, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 813/1000, Step: 12200, Loss: 0.00024924258468672633\n",
            "******************** Test ********************\n",
            "Step: 12200, Loss: 23.157169342041016, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 814/1000, Step: 12220, Loss: 0.0001904589298646897\n",
            "******************** Test ********************\n",
            "Step: 12220, Loss: 23.185775756835938, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 816/1000, Step: 12240, Loss: 0.0003574337752070278\n",
            "******************** Test ********************\n",
            "Step: 12240, Loss: 23.20409393310547, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 817/1000, Step: 12260, Loss: 0.00015718424401711673\n",
            "******************** Test ********************\n",
            "Step: 12260, Loss: 23.204320907592773, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 818/1000, Step: 12280, Loss: 0.001038815244100988\n",
            "******************** Test ********************\n",
            "Step: 12280, Loss: 23.31197166442871, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 820/1000, Step: 12300, Loss: 0.00035358715103939176\n",
            "******************** Test ********************\n",
            "Step: 12300, Loss: 23.329673767089844, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 821/1000, Step: 12320, Loss: 0.0012836218811571598\n",
            "******************** Test ********************\n",
            "Step: 12320, Loss: 23.313013076782227, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 822/1000, Step: 12340, Loss: 0.0003399758134037256\n",
            "******************** Test ********************\n",
            "Step: 12340, Loss: 23.347543716430664, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 824/1000, Step: 12360, Loss: 0.00016639559180475771\n",
            "******************** Test ********************\n",
            "Step: 12360, Loss: 23.401241302490234, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 825/1000, Step: 12380, Loss: 0.00010374898556619883\n",
            "******************** Test ********************\n",
            "Step: 12380, Loss: 23.4421443939209, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 826/1000, Step: 12400, Loss: 0.00013581945677287877\n",
            "******************** Test ********************\n",
            "Step: 12400, Loss: 23.446548461914062, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 828/1000, Step: 12420, Loss: 0.00019676245574373752\n",
            "******************** Test ********************\n",
            "Step: 12420, Loss: 23.474260330200195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 829/1000, Step: 12440, Loss: 0.000666722422465682\n",
            "******************** Test ********************\n",
            "Step: 12440, Loss: 23.499792098999023, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 830/1000, Step: 12460, Loss: 0.000666400883346796\n",
            "******************** Test ********************\n",
            "Step: 12460, Loss: 23.543710708618164, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 832/1000, Step: 12480, Loss: 0.00012493174290284514\n",
            "******************** Test ********************\n",
            "Step: 12480, Loss: 23.567684173583984, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 833/1000, Step: 12500, Loss: 0.0008321813074871898\n",
            "******************** Test ********************\n",
            "Step: 12500, Loss: 23.579452514648438, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 834/1000, Step: 12520, Loss: 0.00012207162217237055\n",
            "******************** Test ********************\n",
            "Step: 12520, Loss: 23.580726623535156, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 836/1000, Step: 12540, Loss: 0.0001378506131004542\n",
            "******************** Test ********************\n",
            "Step: 12540, Loss: 23.611141204833984, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 837/1000, Step: 12560, Loss: 0.0002842108078766614\n",
            "******************** Test ********************\n",
            "Step: 12560, Loss: 23.65688133239746, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 838/1000, Step: 12580, Loss: 0.00025343368179164827\n",
            "******************** Test ********************\n",
            "Step: 12580, Loss: 23.647933959960938, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 840/1000, Step: 12600, Loss: 9.124660573434085e-05\n",
            "******************** Test ********************\n",
            "Step: 12600, Loss: 23.716447830200195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 841/1000, Step: 12620, Loss: 8.119095582515001e-05\n",
            "******************** Test ********************\n",
            "Step: 12620, Loss: 23.73642349243164, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 842/1000, Step: 12640, Loss: 7.73303909227252e-05\n",
            "******************** Test ********************\n",
            "Step: 12640, Loss: 23.733402252197266, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 844/1000, Step: 12660, Loss: 0.0001934679748956114\n",
            "******************** Test ********************\n",
            "Step: 12660, Loss: 23.779863357543945, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 845/1000, Step: 12680, Loss: 0.00015099821030162275\n",
            "******************** Test ********************\n",
            "Step: 12680, Loss: 23.849634170532227, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 846/1000, Step: 12700, Loss: 0.0007970733568072319\n",
            "******************** Test ********************\n",
            "Step: 12700, Loss: 23.78140640258789, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 18s): 848/1000, Step: 12720, Loss: 0.0001258825941476971\n",
            "******************** Test ********************\n",
            "Step: 12720, Loss: 23.826696395874023, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 849/1000, Step: 12740, Loss: 0.00017056558863259852\n",
            "******************** Test ********************\n",
            "Step: 12740, Loss: 23.90530776977539, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 850/1000, Step: 12760, Loss: 0.0004533885221462697\n",
            "******************** Test ********************\n",
            "Step: 12760, Loss: 23.894115447998047, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 852/1000, Step: 12780, Loss: 0.00017148425104096532\n",
            "******************** Test ********************\n",
            "Step: 12780, Loss: 23.91265869140625, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 853/1000, Step: 12800, Loss: 0.0007470559212379158\n",
            "******************** Test ********************\n",
            "Step: 12800, Loss: 23.953718185424805, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 854/1000, Step: 12820, Loss: 0.0002471409097779542\n",
            "******************** Test ********************\n",
            "Step: 12820, Loss: 23.940242767333984, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 856/1000, Step: 12840, Loss: 0.00012016512482659891\n",
            "******************** Test ********************\n",
            "Step: 12840, Loss: 23.92999839782715, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 857/1000, Step: 12860, Loss: 0.0001833192800404504\n",
            "******************** Test ********************\n",
            "Step: 12860, Loss: 23.99164581298828, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 858/1000, Step: 12880, Loss: 0.00047891735448502004\n",
            "******************** Test ********************\n",
            "Step: 12880, Loss: 24.01588249206543, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 860/1000, Step: 12900, Loss: 0.00045227311784401536\n",
            "******************** Test ********************\n",
            "Step: 12900, Loss: 24.07732582092285, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 861/1000, Step: 12920, Loss: 0.00012240043724887073\n",
            "******************** Test ********************\n",
            "Step: 12920, Loss: 24.077783584594727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 862/1000, Step: 12940, Loss: 9.921663149725646e-05\n",
            "******************** Test ********************\n",
            "Step: 12940, Loss: 24.123695373535156, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 864/1000, Step: 12960, Loss: 0.0001357801229460165\n",
            "******************** Test ********************\n",
            "Step: 12960, Loss: 24.1192684173584, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 865/1000, Step: 12980, Loss: 0.0002359772624913603\n",
            "******************** Test ********************\n",
            "Step: 12980, Loss: 24.153392791748047, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 866/1000, Step: 13000, Loss: 8.388343849219382e-05\n",
            "******************** Test ********************\n",
            "Step: 13000, Loss: 24.197946548461914, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 868/1000, Step: 13020, Loss: 0.0005341426003724337\n",
            "******************** Test ********************\n",
            "Step: 13020, Loss: 24.1861629486084, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 869/1000, Step: 13040, Loss: 0.00030090854852460325\n",
            "******************** Test ********************\n",
            "Step: 13040, Loss: 24.206457138061523, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 870/1000, Step: 13060, Loss: 6.448923522839323e-05\n",
            "******************** Test ********************\n",
            "Step: 13060, Loss: 24.263769149780273, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 872/1000, Step: 13080, Loss: 0.00015935661213006824\n",
            "******************** Test ********************\n",
            "Step: 13080, Loss: 24.3149356842041, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 873/1000, Step: 13100, Loss: 6.559175380971283e-05\n",
            "******************** Test ********************\n",
            "Step: 13100, Loss: 24.307527542114258, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 874/1000, Step: 13120, Loss: 0.0004785034107044339\n",
            "******************** Test ********************\n",
            "Step: 13120, Loss: 24.331867218017578, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 876/1000, Step: 13140, Loss: 8.117588731693104e-05\n",
            "******************** Test ********************\n",
            "Step: 13140, Loss: 24.350013732910156, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 877/1000, Step: 13160, Loss: 0.0002333678858121857\n",
            "******************** Test ********************\n",
            "Step: 13160, Loss: 24.391016006469727, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 878/1000, Step: 13180, Loss: 0.00017166376346722245\n",
            "******************** Test ********************\n",
            "Step: 13180, Loss: 24.388721466064453, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 880/1000, Step: 13200, Loss: 0.00010793039837153628\n",
            "******************** Test ********************\n",
            "Step: 13200, Loss: 24.406925201416016, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 881/1000, Step: 13220, Loss: 9.720767411636189e-05\n",
            "******************** Test ********************\n",
            "Step: 13220, Loss: 24.461196899414062, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 882/1000, Step: 13240, Loss: 6.350560579448938e-05\n",
            "******************** Test ********************\n",
            "Step: 13240, Loss: 24.51146125793457, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 19s): 884/1000, Step: 13260, Loss: 0.00013034457515459508\n",
            "******************** Test ********************\n",
            "Step: 13260, Loss: 24.49339485168457, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 885/1000, Step: 13280, Loss: 6.94935442879796e-05\n",
            "******************** Test ********************\n",
            "Step: 13280, Loss: 24.5256290435791, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 886/1000, Step: 13300, Loss: 5.6085147662088275e-05\n",
            "******************** Test ********************\n",
            "Step: 13300, Loss: 24.53888702392578, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 888/1000, Step: 13320, Loss: 0.00031157906050793827\n",
            "******************** Test ********************\n",
            "Step: 13320, Loss: 24.610315322875977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 889/1000, Step: 13340, Loss: 0.0006697175558656454\n",
            "******************** Test ********************\n",
            "Step: 13340, Loss: 24.61701011657715, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 890/1000, Step: 13360, Loss: 8.96551355253905e-05\n",
            "******************** Test ********************\n",
            "Step: 13360, Loss: 24.62856101989746, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 892/1000, Step: 13380, Loss: 0.00016316653636749834\n",
            "******************** Test ********************\n",
            "Step: 13380, Loss: 24.629919052124023, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 893/1000, Step: 13400, Loss: 0.0004322231106925756\n",
            "******************** Test ********************\n",
            "Step: 13400, Loss: 24.663572311401367, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 894/1000, Step: 13420, Loss: 5.908044477109797e-05\n",
            "******************** Test ********************\n",
            "Step: 13420, Loss: 24.693880081176758, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 896/1000, Step: 13440, Loss: 0.00018636745517142117\n",
            "******************** Test ********************\n",
            "Step: 13440, Loss: 24.730791091918945, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 897/1000, Step: 13460, Loss: 0.00017689840751700103\n",
            "******************** Test ********************\n",
            "Step: 13460, Loss: 24.801429748535156, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 898/1000, Step: 13480, Loss: 5.371622683014721e-05\n",
            "******************** Test ********************\n",
            "Step: 13480, Loss: 24.76764488220215, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 900/1000, Step: 13500, Loss: 0.00013363345351535827\n",
            "******************** Test ********************\n",
            "Step: 13500, Loss: 24.761526107788086, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 901/1000, Step: 13520, Loss: 0.00012137399608036503\n",
            "******************** Test ********************\n",
            "Step: 13520, Loss: 24.810216903686523, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 902/1000, Step: 13540, Loss: 0.0002777397457975894\n",
            "******************** Test ********************\n",
            "Step: 13540, Loss: 24.871051788330078, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 904/1000, Step: 13560, Loss: 0.0002925561857409775\n",
            "******************** Test ********************\n",
            "Step: 13560, Loss: 24.885356903076172, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 905/1000, Step: 13580, Loss: 0.00023079289530869573\n",
            "******************** Test ********************\n",
            "Step: 13580, Loss: 24.896400451660156, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 906/1000, Step: 13600, Loss: 0.00046929801465012133\n",
            "******************** Test ********************\n",
            "Step: 13600, Loss: 24.978775024414062, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 908/1000, Step: 13620, Loss: 0.00013662688434123993\n",
            "******************** Test ********************\n",
            "Step: 13620, Loss: 24.9392147064209, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 909/1000, Step: 13640, Loss: 7.18950032023713e-05\n",
            "******************** Test ********************\n",
            "Step: 13640, Loss: 24.979610443115234, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 910/1000, Step: 13660, Loss: 6.810950435465202e-05\n",
            "******************** Test ********************\n",
            "Step: 13660, Loss: 25.020471572875977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 912/1000, Step: 13680, Loss: 6.963736814213917e-05\n",
            "******************** Test ********************\n",
            "Step: 13680, Loss: 25.049564361572266, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 913/1000, Step: 13700, Loss: 0.00012161472113803029\n",
            "******************** Test ********************\n",
            "Step: 13700, Loss: 25.082809448242188, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 914/1000, Step: 13720, Loss: 0.00014025956625118852\n",
            "******************** Test ********************\n",
            "Step: 13720, Loss: 25.076549530029297, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 916/1000, Step: 13740, Loss: 0.00013176277570892125\n",
            "******************** Test ********************\n",
            "Step: 13740, Loss: 25.103551864624023, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 917/1000, Step: 13760, Loss: 0.0003811228380072862\n",
            "******************** Test ********************\n",
            "Step: 13760, Loss: 25.177370071411133, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 918/1000, Step: 13780, Loss: 0.00025512828142382205\n",
            "******************** Test ********************\n",
            "Step: 13780, Loss: 25.136205673217773, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 920/1000, Step: 13800, Loss: 9.991572005674243e-05\n",
            "******************** Test ********************\n",
            "Step: 13800, Loss: 25.184507369995117, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 921/1000, Step: 13820, Loss: 0.00011953454668400809\n",
            "******************** Test ********************\n",
            "Step: 13820, Loss: 25.177112579345703, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 922/1000, Step: 13840, Loss: 0.00027828742167912424\n",
            "******************** Test ********************\n",
            "Step: 13840, Loss: 25.22456932067871, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 20s): 924/1000, Step: 13860, Loss: 9.709903679322451e-05\n",
            "******************** Test ********************\n",
            "Step: 13860, Loss: 25.24164581298828, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 925/1000, Step: 13880, Loss: 0.00014924121205694973\n",
            "******************** Test ********************\n",
            "Step: 13880, Loss: 25.241907119750977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 926/1000, Step: 13900, Loss: 0.00011082241690019146\n",
            "******************** Test ********************\n",
            "Step: 13900, Loss: 25.27675437927246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 928/1000, Step: 13920, Loss: 8.379477367270738e-05\n",
            "******************** Test ********************\n",
            "Step: 13920, Loss: 25.341188430786133, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 929/1000, Step: 13940, Loss: 3.9723625377519056e-05\n",
            "******************** Test ********************\n",
            "Step: 13940, Loss: 25.368793487548828, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 930/1000, Step: 13960, Loss: 3.398868648218922e-05\n",
            "******************** Test ********************\n",
            "Step: 13960, Loss: 25.359487533569336, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 932/1000, Step: 13980, Loss: 0.0002654040581546724\n",
            "******************** Test ********************\n",
            "Step: 13980, Loss: 25.41522789001465, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 933/1000, Step: 14000, Loss: 0.00022610675659961998\n",
            "******************** Test ********************\n",
            "Step: 14000, Loss: 25.4578914642334, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 934/1000, Step: 14020, Loss: 5.182448512641713e-05\n",
            "******************** Test ********************\n",
            "Step: 14020, Loss: 25.451799392700195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 936/1000, Step: 14040, Loss: 9.332933404948562e-05\n",
            "******************** Test ********************\n",
            "Step: 14040, Loss: 25.481287002563477, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 937/1000, Step: 14060, Loss: 0.00011585881293285638\n",
            "******************** Test ********************\n",
            "Step: 14060, Loss: 25.521770477294922, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 938/1000, Step: 14080, Loss: 5.9393642004579306e-05\n",
            "******************** Test ********************\n",
            "Step: 14080, Loss: 25.570899963378906, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 940/1000, Step: 14100, Loss: 0.00011348714178893715\n",
            "******************** Test ********************\n",
            "Step: 14100, Loss: 25.584247589111328, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 941/1000, Step: 14120, Loss: 2.8281576305744238e-05\n",
            "******************** Test ********************\n",
            "Step: 14120, Loss: 25.5855655670166, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 942/1000, Step: 14140, Loss: 0.00023996045638341457\n",
            "******************** Test ********************\n",
            "Step: 14140, Loss: 25.644983291625977, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 944/1000, Step: 14160, Loss: 5.3012470743851736e-05\n",
            "******************** Test ********************\n",
            "Step: 14160, Loss: 25.64061737060547, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 945/1000, Step: 14180, Loss: 0.00020780057820957154\n",
            "******************** Test ********************\n",
            "Step: 14180, Loss: 25.694368362426758, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 946/1000, Step: 14200, Loss: 5.6472752476111054e-05\n",
            "******************** Test ********************\n",
            "Step: 14200, Loss: 25.71196937561035, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 948/1000, Step: 14220, Loss: 4.698180782725103e-05\n",
            "******************** Test ********************\n",
            "Step: 14220, Loss: 25.736740112304688, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 949/1000, Step: 14240, Loss: 0.0001902682997751981\n",
            "******************** Test ********************\n",
            "Step: 14240, Loss: 25.769071578979492, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 950/1000, Step: 14260, Loss: 0.00011210305092390627\n",
            "******************** Test ********************\n",
            "Step: 14260, Loss: 25.783872604370117, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 952/1000, Step: 14280, Loss: 0.00010173734335694462\n",
            "******************** Test ********************\n",
            "Step: 14280, Loss: 25.79242515563965, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 953/1000, Step: 14300, Loss: 6.251907325349748e-05\n",
            "******************** Test ********************\n",
            "Step: 14300, Loss: 25.8376522064209, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 954/1000, Step: 14320, Loss: 0.00023947849695105106\n",
            "******************** Test ********************\n",
            "Step: 14320, Loss: 25.841215133666992, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 956/1000, Step: 14340, Loss: 8.117692050291225e-05\n",
            "******************** Test ********************\n",
            "Step: 14340, Loss: 25.90131187438965, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 957/1000, Step: 14360, Loss: 8.14135855762288e-05\n",
            "******************** Test ********************\n",
            "Step: 14360, Loss: 25.93817138671875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 958/1000, Step: 14380, Loss: 0.0001320079027209431\n",
            "******************** Test ********************\n",
            "Step: 14380, Loss: 25.95303726196289, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 960/1000, Step: 14400, Loss: 0.00021162517077755183\n",
            "******************** Test ********************\n",
            "Step: 14400, Loss: 25.9610652923584, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 961/1000, Step: 14420, Loss: 3.020351505256258e-05\n",
            "******************** Test ********************\n",
            "Step: 14420, Loss: 26.004613876342773, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 962/1000, Step: 14440, Loss: 3.580606062314473e-05\n",
            "******************** Test ********************\n",
            "Step: 14440, Loss: 26.010169982910156, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 964/1000, Step: 14460, Loss: 0.00018744653789326549\n",
            "******************** Test ********************\n",
            "Step: 14460, Loss: 26.022546768188477, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 21s): 965/1000, Step: 14480, Loss: 0.00011042022379115224\n",
            "******************** Test ********************\n",
            "Step: 14480, Loss: 26.08049964904785, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 966/1000, Step: 14500, Loss: 8.455592615064234e-05\n",
            "******************** Test ********************\n",
            "Step: 14500, Loss: 26.057205200195312, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 968/1000, Step: 14520, Loss: 0.00011323420039843768\n",
            "******************** Test ********************\n",
            "Step: 14520, Loss: 26.088266372680664, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 969/1000, Step: 14540, Loss: 3.732597906491719e-05\n",
            "******************** Test ********************\n",
            "Step: 14540, Loss: 26.159807205200195, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 970/1000, Step: 14560, Loss: 4.499793431023136e-05\n",
            "******************** Test ********************\n",
            "Step: 14560, Loss: 26.160158157348633, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 972/1000, Step: 14580, Loss: 0.0003112155827693641\n",
            "******************** Test ********************\n",
            "Step: 14580, Loss: 26.202526092529297, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 973/1000, Step: 14600, Loss: 0.00029637719853781164\n",
            "******************** Test ********************\n",
            "Step: 14600, Loss: 26.194995880126953, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 974/1000, Step: 14620, Loss: 2.8117765396018513e-05\n",
            "******************** Test ********************\n",
            "Step: 14620, Loss: 26.241331100463867, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 976/1000, Step: 14640, Loss: 7.454337173840031e-05\n",
            "******************** Test ********************\n",
            "Step: 14640, Loss: 26.246854782104492, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 977/1000, Step: 14660, Loss: 6.293789192568511e-05\n",
            "******************** Test ********************\n",
            "Step: 14660, Loss: 26.260690689086914, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 978/1000, Step: 14680, Loss: 5.49960823263973e-05\n",
            "******************** Test ********************\n",
            "Step: 14680, Loss: 26.32914924621582, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 980/1000, Step: 14700, Loss: 6.521611794596538e-05\n",
            "******************** Test ********************\n",
            "Step: 14700, Loss: 26.324968338012695, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 981/1000, Step: 14720, Loss: 1.6882617273950018e-05\n",
            "******************** Test ********************\n",
            "Step: 14720, Loss: 26.350936889648438, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 982/1000, Step: 14740, Loss: 3.7489819078473374e-05\n",
            "******************** Test ********************\n",
            "Step: 14740, Loss: 26.403352737426758, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 984/1000, Step: 14760, Loss: 4.422424899530597e-05\n",
            "******************** Test ********************\n",
            "Step: 14760, Loss: 26.408361434936523, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 985/1000, Step: 14780, Loss: 2.1978663426125422e-05\n",
            "******************** Test ********************\n",
            "Step: 14780, Loss: 26.428569793701172, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 986/1000, Step: 14800, Loss: 9.927424980560318e-05\n",
            "******************** Test ********************\n",
            "Step: 14800, Loss: 26.47263526916504, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 988/1000, Step: 14820, Loss: 6.762905832147226e-05\n",
            "******************** Test ********************\n",
            "Step: 14820, Loss: 26.518028259277344, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 989/1000, Step: 14840, Loss: 0.00011122575961053371\n",
            "******************** Test ********************\n",
            "Step: 14840, Loss: 26.51130485534668, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 990/1000, Step: 14860, Loss: 0.00021024458692409098\n",
            "******************** Test ********************\n",
            "Step: 14860, Loss: 26.48979377746582, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 992/1000, Step: 14880, Loss: 5.5115873692557216e-05\n",
            "******************** Test ********************\n",
            "Step: 14880, Loss: 26.54898452758789, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 993/1000, Step: 14900, Loss: 0.00014305344666354358\n",
            "******************** Test ********************\n",
            "Step: 14900, Loss: 26.58534812927246, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 994/1000, Step: 14920, Loss: 4.0350667404709384e-05\n",
            "******************** Test ********************\n",
            "Step: 14920, Loss: 26.610076904296875, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 996/1000, Step: 14940, Loss: 3.272205503890291e-05\n",
            "******************** Test ********************\n",
            "Step: 14940, Loss: 26.645986557006836, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 997/1000, Step: 14960, Loss: 0.0001286656188312918\n",
            "******************** Test ********************\n",
            "Step: 14960, Loss: 26.64741325378418, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n",
            "Epoch(0m 22s): 998/1000, Step: 14980, Loss: 0.00014425668632611632\n",
            "******************** Test ********************\n",
            "Step: 14980, Loss: 26.700801849365234, test accuracy: 15.789473684210526 %\n",
            "**********************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "328KSQnXnEAj",
        "outputId": "c670232f-9c2c-48dc-cba9-4d82345b5a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 2269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa92f6f9198>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2269
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rujZMeJnF2b",
        "outputId": "306ce2f4-8840-41b5-f8e8-9f7bf6b5da1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test\n",
        "model.eval()\n",
        "acc = 0.\n",
        "step = 0\n",
        "with torch.no_grad():\n",
        "    for idx, (x, y) in enumerate(test_loader):\n",
        "        y_hat = model(x) # (N, 21)\n",
        "        loss = loss_fn(y_hat, y)\n",
        "        \n",
        "        _, indices = torch.max(y_hat, dim=-1)     \n",
        "        acc += torch.sum(indices == y).item()\n",
        "        #print(indices)\n",
        "        #print(y)     \n",
        "\n",
        "        step = step+1\n",
        "print('*'*20, 'Test', '*'*20)\n",
        "print('Step: {}, Loss: {}, Accuracy: {}%'.format(step, loss.item(), acc/len(test_dataset)*100))\n",
        "print('*'*46)"
      ],
      "execution_count": 2270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([11, 11, 15, 12, 16, 10, 10,  8, 10, 15, 15, 12, 13, 15,  0, 11, 12, 12,\n",
            "        14, 14, 12, 12,  0, 12,  0, 15, 12, 14, 13, 15, 15, 13, 11, 12, 11, 13,\n",
            "         0])\n",
            "tensor([13, 10, 17, 11, 15, 14, 13,  9,  8, 14,  5,  8, 10, 11,  0, 11, 10, 10,\n",
            "        14, 11, 16, 12,  6, 15,  9,  6, 15,  8,  9,  0, 15, 10, 11, 10, 11,  8,\n",
            "        10])\n",
            "******************** Test ********************\n",
            "Step: 1, Loss: 29.355297088623047, Accuracy: 18.91891891891892%\n",
            "**********************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWlTQlBnPuDu"
      },
      "source": [
        "\n"
      ],
      "execution_count": 2270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoGura49O_OY"
      },
      "source": [
        ""
      ],
      "execution_count": 2270,
      "outputs": []
    }
  ]
}