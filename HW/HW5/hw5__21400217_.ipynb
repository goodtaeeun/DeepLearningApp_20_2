{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw5_<21400217>",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIAvwlvKzyFf",
        "outputId": "4940ddd0-23ab-4292-cec6-06cdcaa7b102"
      },
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torchvision            \n",
        "import torch.nn as nn\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwbPq-60N12"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MGNpFjC0T-m"
      },
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torchvision"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKmpdDZs0UqL"
      },
      "source": [
        "#import matplotlib\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib.pyplot import imshow, imsave"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8KGb8Uk0W4M",
        "outputId": "f23cbfae-4e52-4b19-aed3-481060f9c606"
      },
      "source": [
        "MODEL_NAME = 'CNN'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"MODEL_NAME = {}, DEVICE = {}\".format(MODEL_NAME, DEVICE))"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL_NAME = CNN, DEVICE = cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyy8vVdgMcU3"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor() ])                              # image to tensor"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWsWJ7NP0-qS"
      },
      "source": [
        " # create and use a transform\n",
        " mnist_train = datasets.MNIST(root='./data/', train=True,transform=transform, download=True)\n",
        " mnist_test = datasets.MNIST(root='./data/', train=False,transform=transform, download=True)\n",
        "\n",
        "# create and use a transform\n",
        "fashion_train = datasets.FashionMNIST('./data/', download=True, train=True, transform=transform)\n",
        "fashion_test = datasets.FashionMNIST('./data/', download=True, train=False, transform=transform)"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePKAHZk41mBB",
        "outputId": "8c88ff67-6c01-4513-97ee-b5dc2eee5ff4"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2iTgaiE1mkg"
      },
      "source": [
        "## Creating Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6kr5Fk51qxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb97150-b83b-4780-8631-a71363003cb2"
      },
      "source": [
        "!mkdir \"./train\"\n",
        "!mkdir \"./test\"\n",
        "\n",
        "for i in range(20):\n",
        "    train = \"./train/\" + str(i)\n",
        "    test = \"./test/\" + str(i)\n",
        "    !mkdir $train\n",
        "    !mkdir $test\n",
        "\n"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./train’: File exists\n",
            "mkdir: cannot create directory ‘./test’: File exists\n",
            "mkdir: cannot create directory ‘./train/0’: File exists\n",
            "mkdir: cannot create directory ‘./test/0’: File exists\n",
            "mkdir: cannot create directory ‘./train/1’: File exists\n",
            "mkdir: cannot create directory ‘./test/1’: File exists\n",
            "mkdir: cannot create directory ‘./train/2’: File exists\n",
            "mkdir: cannot create directory ‘./test/2’: File exists\n",
            "mkdir: cannot create directory ‘./train/3’: File exists\n",
            "mkdir: cannot create directory ‘./test/3’: File exists\n",
            "mkdir: cannot create directory ‘./train/4’: File exists\n",
            "mkdir: cannot create directory ‘./test/4’: File exists\n",
            "mkdir: cannot create directory ‘./train/5’: File exists\n",
            "mkdir: cannot create directory ‘./test/5’: File exists\n",
            "mkdir: cannot create directory ‘./train/6’: File exists\n",
            "mkdir: cannot create directory ‘./test/6’: File exists\n",
            "mkdir: cannot create directory ‘./train/7’: File exists\n",
            "mkdir: cannot create directory ‘./test/7’: File exists\n",
            "mkdir: cannot create directory ‘./train/8’: File exists\n",
            "mkdir: cannot create directory ‘./test/8’: File exists\n",
            "mkdir: cannot create directory ‘./train/9’: File exists\n",
            "mkdir: cannot create directory ‘./test/9’: File exists\n",
            "mkdir: cannot create directory ‘./train/10’: File exists\n",
            "mkdir: cannot create directory ‘./test/10’: File exists\n",
            "mkdir: cannot create directory ‘./train/11’: File exists\n",
            "mkdir: cannot create directory ‘./test/11’: File exists\n",
            "mkdir: cannot create directory ‘./train/12’: File exists\n",
            "mkdir: cannot create directory ‘./test/12’: File exists\n",
            "mkdir: cannot create directory ‘./train/13’: File exists\n",
            "mkdir: cannot create directory ‘./test/13’: File exists\n",
            "mkdir: cannot create directory ‘./train/14’: File exists\n",
            "mkdir: cannot create directory ‘./test/14’: File exists\n",
            "mkdir: cannot create directory ‘./train/15’: File exists\n",
            "mkdir: cannot create directory ‘./test/15’: File exists\n",
            "mkdir: cannot create directory ‘./train/16’: File exists\n",
            "mkdir: cannot create directory ‘./test/16’: File exists\n",
            "mkdir: cannot create directory ‘./train/17’: File exists\n",
            "mkdir: cannot create directory ‘./test/17’: File exists\n",
            "mkdir: cannot create directory ‘./train/18’: File exists\n",
            "mkdir: cannot create directory ‘./test/18’: File exists\n",
            "mkdir: cannot create directory ‘./train/19’: File exists\n",
            "mkdir: cannot create directory ‘./test/19’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skVUJXUVb4KF"
      },
      "source": [
        "# saving training images\n",
        "\n",
        "train_count = np.zeros(20)\n",
        "\n",
        "for mnist_index in range(10):\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while j < 2000:\n",
        "        temp_image, temp_lable = mnist_train[i]\n",
        "        i = i + 1\n",
        "        if temp_lable == mnist_index:\n",
        "            path = \"./train/\" + str(temp_lable) + \"/\" + str(j) + \".jpg\"\n",
        "            torchvision.utils.save_image(temp_image, path)\n",
        "            train_count[mnist_index] = j\n",
        "            j = j + 1\n",
        "            \n",
        "\n",
        "for fashion_index in range(10):\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while j < 2000:\n",
        "        temp_image, temp_lable = fashion_train[i]\n",
        "        i = i + 1\n",
        "        if temp_lable == fashion_index:\n",
        "            path = \"./train/\" + str(temp_lable+10) + \"/\" + str(j) + \".jpg\"\n",
        "            torchvision.utils.save_image(temp_image, path)\n",
        "            train_count[fashion_index+10] = j\n",
        "            j = j + 1\n",
        "            \n"
      ],
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Efite2kZv8"
      },
      "source": [
        "# saving test images\n",
        "\n",
        "test_count = np.zeros(20)\n",
        "\n",
        "for mnist_index in range(10):\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while j < 1000 and i < 10000:\n",
        "        temp_image, temp_lable = mnist_test[i]\n",
        "        i = i + 1\n",
        "        if temp_lable == mnist_index:\n",
        "            path = \"./test/\" + str(temp_lable) + \"/\" + str(j) + \".jpg\"\n",
        "            torchvision.utils.save_image(temp_image, path)\n",
        "            test_count[mnist_index] = j\n",
        "            j = j + 1\n",
        "            \n",
        "\n",
        "for fashion_index in range(10):\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while j < 1000 and i < 10000:\n",
        "        temp_image, temp_lable = fashion_test[i]\n",
        "        i = i + 1\n",
        "        if temp_lable == fashion_index:\n",
        "            path = \"./test/\" + str(temp_lable+10) + \"/\" + str(j) + \".jpg\"\n",
        "            torchvision.utils.save_image(temp_image, path)\n",
        "            test_count[fashion_index+10] = j\n",
        "            j = j + 1\n",
        "            "
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Vh4sqrp7V6"
      },
      "source": [
        "## Dateset 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_04hZQ9p6UM"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "from skimage import data\n"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua4P72Bsp-Iq"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, train, instance_count, root_dir, transform=None):\n",
        "        \n",
        "        self.train = train\n",
        "        self.instance_count = instance_count\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        if train:\n",
        "            path = \"./train/\"\n",
        "        else:\n",
        "            path = \"./test/\"\n",
        "\n",
        "        # 이미지 읽어오기\n",
        "        self.samples = []\n",
        "        for lable_index in range(20):\n",
        "            j = 0\n",
        "            while j < instance_count[lable_index]:\n",
        "                temp_image = io.imread(path + str(lable_index) + \"/\" + str(j) + \".jpg\", as_gray=True)\n",
        "                self.samples.append((temp_image, lable_index))\n",
        "                j = j + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.instance_count.sum())\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.transform:\n",
        "            trans_image, lable = self.samples[idx]\n",
        "            trans_image = self.transform(trans_image)\n",
        "            return (trans_image, lable)\n",
        "\n",
        "        return self.samples[idx]"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDnGkggZGiB9"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),                               # image to tensor\n",
        "     transforms.Normalize(mean=(0.1307,), std=(0.3081,))  # normalize to \"(x-mean)/std\"\n",
        "    ])"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJbQIipVIh9L"
      },
      "source": [
        "train_dataset = MyDataset(train = True, instance_count = train_count, root_dir = \"./Mydata/train/\", transform = transform)\n",
        "test_dataset = MyDataset(train = False, instance_count = test_count, root_dir = \"./Mydata/test/\", transform = transform)\n",
        "\n"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGSNYnI2Tdg4"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP7_vZOnHuUe"
      },
      "source": [
        "batch_size = 1000\n"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xsBXzN0HpQZ"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False, drop_last=False)"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2_2UTtEHy7X"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8pTJlkhLnef"
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple CNN Clssifier\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=20):\n",
        "        super(MyCNN, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            # (N, 1, 28, 28) 여기서 N은 배치 사이즈\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, dilation =3, padding=4), # 2차원 convolution layer\n",
        "            # 얘의 결과는 (N, 64, 28, 28),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), \n",
        "            # MAx Pooling 결과는  (N, 64, 14, 14)\n",
        "\n",
        "            # (N, 64, 14, 14)\n",
        "            nn.Conv2d(64, 512, kernel_size=3, stride=1, padding=1),\n",
        "            # 결과는 (N, 512, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            # (N, 512, 7 , 7) 배치, 채널, height, width\n",
        "\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "            # 결과는 (N, 1024, 7, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 1),\n",
        "            # (N, 1024, 6, 6) 배치, 채널, height, width\n",
        "\n",
        "        )\n",
        "        self.fc = nn.Sequential( # 여기부터 fully connected라서 바꿔줘야 함, 여기부터는 2차원\n",
        "            nn.Linear(6*6*1024, 512), # 512는 output node\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y_ = self.conv(x) # (N, 64, 7, 7) # cnn은 input이 삼차원이니까 그대로 넣어준다.\n",
        "        y_ = y_.view(y_.size(0), -1) # (N, 64*7*7) # 3차원을 일렬로 된 모양으로 바꿔준다.\n",
        "        y_ = self.fc(y_)\n",
        "        return y_"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4ji5ChSRId"
      },
      "source": [
        "## 모델 설명\r\n",
        "\r\n",
        "우선 Fashion Mnist 와 Mnist의 이미지를 실제로 보면 가장 큰 차이는 하얀 픽셀들의 밀집도이다. 즉, 숫자 이미지인 mnist 데이터셋은 그림의 영역을 나타내는 부분들이 더 얇게 분포되어있고, 옷 이미지인 Fashion Mnist는 좀 더 밀집되어 두꺼운 분포를 가진다. 따라서 이런 큰 카테고리 상에서의 차이를 학습하려면 커널의 사이즈가 어느 정도 커야 그 context를 파악할 수 있을 것이다. 그러나 커널을 키워버리면 적은 파라미터라는 CNN의 장점이 없어진다. 따라서 작은 커널로도 넓은 Context를 확보할 수 있는 dilated convolution layer를 첫 Layer로 사용하였다.\r\n",
        "\r\n",
        "Batch의 경우 현재 데이터 셋에 mnist와 fashion dataset이 섞여있기 때문에 너무 작은 사이즈를 설정한다면 두 데이터셋의 분포가 고르지 않을 상황을 염려하여 최대한 큰 사이즈로 설정하였다. 대신에 학습 시에 epoch을 늘려서 학습 횟수를 보장하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp0pklhQLqT0"
      },
      "source": [
        "model = MyCNN().to(DEVICE)"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmib3lyUHzrM"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxmG3EVEMIcs"
      },
      "source": [
        "all_losses = []"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_kJ59RQMuzn",
        "outputId": "e2411cc0-9a1d-42f7-8b7f-128805f20348"
      },
      "source": [
        "max_epoch = 50        # maximum number of epochs\n",
        "step = 0             # initialize step counter variable\n",
        "\n",
        "plot_every = 200\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "        # Training Discriminator\n",
        "        x, y = images.to(DEVICE,  dtype = torch.float), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
        "        \n",
        "        #print(x.dtype)\n",
        "        y_hat = model(x)      # (N, 10)  # forward propagation\n",
        "       \n",
        "        loss = criterion(y_hat, y)  # computing loss\n",
        "        total_loss += loss.item()\n",
        "          \n",
        "        optim.zero_grad()           # reset gradient\n",
        "        loss.backward()             # back-propagation (compute gradient)\n",
        "        optim.step()                # update parameters with gradient\n",
        "        \n",
        "        # periodically print loss\n",
        "        if step % 120 == 0:\n",
        "            print('Epoch({}): {}/{}, Step: {}, Loss: {}'.format(timeSince(start), epoch, max_epoch, step, loss.item()))\n",
        "        \n",
        "        if (step + 1) % plot_every == 0:\n",
        "            all_losses.append(total_loss / plot_every)\n",
        "            total_loss = 0\n",
        "        \n",
        "        # periodically evalute model on test data\n",
        "        if step % 250 == 0:\n",
        "            model.eval()\n",
        "            acc = 0.\n",
        "            with torch.no_grad():   # disable autograd\n",
        "                for idx, (images, labels) in enumerate(test_loader):\n",
        "                    x, y = images.to(DEVICE, dtype=torch.float), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
        "                    \n",
        "                    y_hat = model(x) # (N, 10)\n",
        "                    loss = criterion(y_hat, y)\n",
        "                    _, indices = torch.max(y_hat, dim=-1)     # find maxmum along the last axis (argmax of each row)\n",
        "                                                              # ex) max_value, max_idx = torch.max(input, dim)\n",
        "                    acc += torch.sum(indices == y).item()     # count correctly classified samples\n",
        "                                                              # torch.sum() returns Tensor. Tensor.item() converts it to a value\n",
        "            print('*'*20, 'Test', '*'*20)\n",
        "            print('Step: {}, Loss: {}, Accuracy: {} %'.format(step, loss.item(), acc/test_count.sum()*100))\n",
        "            print('*'*46)\n",
        "            model.train()           # turn to train mode (enable autograd)\n",
        "        step += 1"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch(0m 0s): 0/50, Step: 0, Loss: 3.0023140907287598\n",
            "******************** Test ********************\n",
            "Step: 0, Loss: 0.007223741617053747, Accuracy: 12.440554487503794 %\n",
            "**********************************************\n",
            "Epoch(0m 24s): 3/50, Step: 120, Loss: 0.2447749674320221\n",
            "Epoch(0m 45s): 6/50, Step: 240, Loss: 0.18030129373073578\n",
            "******************** Test ********************\n",
            "Step: 250, Loss: 0.08731666952371597, Accuracy: 92.68440756855206 %\n",
            "**********************************************\n",
            "Epoch(1m 9s): 9/50, Step: 360, Loss: 0.11911100149154663\n",
            "Epoch(1m 30s): 12/50, Step: 480, Loss: 0.11614209413528442\n",
            "******************** Test ********************\n",
            "Step: 500, Loss: 0.11552900820970535, Accuracy: 93.72660123444298 %\n",
            "**********************************************\n",
            "Epoch(1m 55s): 15/50, Step: 600, Loss: 0.05544350668787956\n",
            "Epoch(2m 16s): 18/50, Step: 720, Loss: 0.06652700155973434\n",
            "******************** Test ********************\n",
            "Step: 750, Loss: 0.05270072817802429, Accuracy: 94.2527572599413 %\n",
            "**********************************************\n",
            "Epoch(2m 40s): 21/50, Step: 840, Loss: 0.040157027542591095\n",
            "Epoch(3m 1s): 24/50, Step: 960, Loss: 0.0321725495159626\n",
            "******************** Test ********************\n",
            "Step: 1000, Loss: 0.021550819277763367, Accuracy: 93.93908732166346 %\n",
            "**********************************************\n",
            "Epoch(3m 26s): 27/50, Step: 1080, Loss: 0.03679397702217102\n",
            "Epoch(3m 47s): 30/50, Step: 1200, Loss: 0.024114497005939484\n",
            "******************** Test ********************\n",
            "Step: 1250, Loss: 0.10272419452667236, Accuracy: 94.17686937164828 %\n",
            "**********************************************\n",
            "Epoch(4m 11s): 33/50, Step: 1320, Loss: 0.022645730525255203\n",
            "Epoch(4m 32s): 36/50, Step: 1440, Loss: 0.014110049232840538\n",
            "******************** Test ********************\n",
            "Step: 1500, Loss: 0.13852357864379883, Accuracy: 94.0301527876151 %\n",
            "**********************************************\n",
            "Epoch(4m 57s): 40/50, Step: 1560, Loss: 0.006477757357060909\n",
            "Epoch(5m 18s): 43/50, Step: 1680, Loss: 0.011903002858161926\n",
            "******************** Test ********************\n",
            "Step: 1750, Loss: 0.06939391791820526, Accuracy: 93.918850551452 %\n",
            "**********************************************\n",
            "Epoch(5m 42s): 46/50, Step: 1800, Loss: 0.010307461023330688\n",
            "Epoch(6m 3s): 49/50, Step: 1920, Loss: 0.003688911907374859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "v-65JsIoMyTO",
        "outputId": "899836ab-03fe-4a82-e002-43cd69ebfe67"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fad5ef5f1d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob75iYEQM0m5",
        "outputId": "a863e797-2212-4fe9-d3d4-6a17b00cbd02"
      },
      "source": [
        "# Test\n",
        "model.eval()\n",
        "acc = 0.\n",
        "with torch.no_grad():\n",
        "    for idx, (images, labels) in enumerate(test_loader):\n",
        "        x, y = images.to(DEVICE, dtype=torch.float), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
        "        y_hat = model(x) # (N, 10)\n",
        "        loss = criterion(y_hat, y)\n",
        "        _, indices = torch.max(y_hat, dim=-1)\n",
        "        acc += torch.sum(indices == y).item()\n",
        "print('*'*20, 'Test', '*'*20)\n",
        "print('Step: {}, Loss: {}, Accuracy: {} %'.format(step, loss.item(), acc/test_count.sum()*100))\n",
        "print('*'*46)"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************** Test ********************\n",
            "Step: 1950, Loss: 0.15849801898002625, Accuracy: 94.10604067590812 %\n",
            "**********************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItbOhy6iM-8m"
      },
      "source": [
        ""
      ],
      "execution_count": 287,
      "outputs": []
    }
  ]
}